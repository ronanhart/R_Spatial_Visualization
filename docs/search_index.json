[["index.html", "R Spatial and Visualization Workshop Chapter 1 Welcome to the R Spatial and Visualization Workshop! Table of Contents Goals of this workshop: Notes on Updates Why GIS in R? Prerequisites, Packages, and Preparation", " R Spatial and Visualization Workshop Ronan Hart 2023-09-08 Chapter 1 Welcome to the R Spatial and Visualization Workshop! (This workshop was originally created for the Utah State University’s Ecology Center’s R Workshop Series) Table of Contents Chapter 1: Introduction and Prerequisites Chapter 2: Introduction to GIS This chapter is recommended if you have never taken any GIS course, it is your first time working with spatial data, or you would like a refresher on the basics of GIS. Chapter 3: Spatial Data in R This chapter is recommended if you have worked with spatial data in other programs or software such as the ArcGIS suite but are unfamiliar with how to load or manipulate spatial data in R. Chapter 4: Spatial Analysis This is the meat of this workshop. This chapter goes over how to perform some spatial analyses you are likely going to use. This is not comprehensive to every possible function or analysis that one might need. Chapter 5: Visualization Once you’ve performed your spatial analysis, you likely need to make some nice-looking maps to include in a manuscript or presentation. This chapter goes over some tips and tricks to make your maps a little more visually appealing, such as how to adjust the color palettes, add map elements like a north arrow and scale bar, including an inset, and more. Chapter 6: Conclusions and Resources This brief chapter offers some resources on where to find spatial data you may need in your analyses and where I obtained the data used in the workshop. Goals of this workshop: Reiterate the basics of GIS Teach you the processes that you might have learned in ArcGIS or other GIS software that you can code yourself in R Teach you some tips and tricks to make pretty maps If you would like to follow along, try out some of the examples, and work on some practice exercises, go to this workshop’s github page and download the data from the Data folder and the “worksheet” page called worksheet.R Notes on Updates Chapters 4 and 5 are certainly not comprehensive to every function, analysis, or option for performing spatial analysis or creating pretty maps. If you have a suggestion to something specific that either you use often or something you need to use but don’t know how to do it in R, please contact me and I am happy to add it! Please note that this workshop mainly covers how to make maps using ggplot2 and not base R plotting. It is my goal to one day include these options as well. However, 1) I have only ever plotted and made maps with ggplot2, so I will first have to familiarize myself with base R plotting, which may take a bit of time; and 2) I made this in my free-time as a graduate student and now a full-time worker, so updates will happen slowly. Why GIS in R? You may be asking why you even need to learn how to code spatial processes in R, especially if you already know how to use ArcGIS. (Well, maybe you’re not actually asking that question if you’re taking this class.) But here are a few reasons why: Free you most often need to pay companies to use their GIS software (or be a student to use a university’s license). What happens when you’re no longer a student nor hired by a company/organization that already has a license? Reproducible Some journals require you to publish your code alongside your manuscript If you are collaborating on a project, you can easily share your methods and scripts with your collaborators. Open-Source company-owned software is often hidden behind a “black box” so you might not be 100% certain what a function or tool is doing. Functions in R are completely open to the public so you can be certain that a function is doing what you think it’s doing Customizable You can write your code to suit your specific problem and answer your specific questions You can write custom functions and loops so that you can repeat processes on multiple features and rasters without having to copy and paste code Reproducibility and customization are not unique to R but rather an advantage to using code for GIS in general. In a few weeks there will be a workshop on coding in python for GIS tools, which is just as useful (especially because you can use the package arcpy in python to code directly to ArcGIS), so I also recommend taking that workshop if you’re interested. Not to say that programs such as ArcGIS should never be used. On the contrary, since it was the way I first learned GIS, I will sometimes return to it to make a map on the fly or quickly visualize and double-check a polygon or raster. All programs have their pros and cons, and this workshop is to simply add another tool in your spatial analysis toolbox. Prerequisites, Packages, and Preparation Before we begin, please be sure you have done the follow preparations: Make sure you have a recent version of R and RStudio installed to check your version of R, type R.version in the console. version.string will have the version number and its release date. The most recent version (as of 2022-03-30) is 4.1.3 (released on 2022-03-10). Version 4.1.2 is perfectly fine for this workshop. to check your version of RStudio, Go to Help (in the toolbar at the top) &gt; Check for Updates. RStudio 2022.02.1+461 is the most recent version. Version 2021.09.2+382 is perfectly fine for this workshop. Install (if needed) and load the following packages: install.packages(&quot;terra&quot;) install.packages(&quot;sf&quot;) install.packages(&quot;tidyverse&quot;) library(terra) library(sf) library(tidyverse) (Optional but recommended) Create a new RStudio project. This will make it easier to access files within your code and keep everything organized. Here’s a guide on how to do that After taking care of that, let’s get started! "],["gis-basics.html", "Chapter 2 GIS basics 2.1 Datums, Projections, and Coordinate Systems 2.2 Spatial Data 2.3 Vectors vs Rasters: pros &amp; cons", " Chapter 2 GIS basics 2.1 Datums, Projections, and Coordinate Systems Datums The Earth is a spheroid (also called an ellipsoid). Because of variations in elevation across the world, the Earth’s surface is irregular. Figure 2.1: Conceptual representation of the irregular, spheroid shape of the Earth A datum (also called a geographic coordinate system) is a reference surface that best fits the mean surface area of an area of interest. There is a global datum to represent the general surface of the Earth as a whole — World Geodetic System of 1984 i.e. WGS84. Figure 2.2: Red ellipse represents the smooth, general surface of the Earth (i.e. a global datum) However, because the Earth’s surface is irregular and the global datum might not reflect specific areas and variations in elevations, there are also local datums. A common local datum (for North America at least) is the North America Datum of 1983 (NAD83) . Figure 2.3: Yellow line indicates a specific area, purple ellipse represents the smooth, general surface of the Earth at this location. Note that this local datum would not be a best fit in other places on the Earth The datum you choose to work with is up to you and where your study takes place. It is very important to know what datum you’re working with and that you remain consistent because coordinates of a location from one datum are likely different than the same location from a different datum. For example, if we look at the coordinates for Bellingham, Washington: Datum Longitude Latitude NAD 1983 -122.46818353793 48.7438798543649 WGS 1984 -122.46818353793 48.7438798534299 While the differences between NAD83 and WGS84 are not huge, these differences could impact any spatial analysis you perform. Also note that you would need to choose a different local datum if you’re working outside of North America. Projections While a datum references the position of an object in geographic space on a 3D surface, a projection (also called a projected coordinate system) represents that 3D surface onto a 2D plane. Figure 2.4: Conceptual demonstration of map projections This is important to know when plotting a map for a figure, as your chosen projection will change the visualization and shape of your map’s features. But more importantly for spatial analysis, a projection is needed when you need values such as length, area, or distance. Map projections are never 100% accurate, so every 2D map will have show some distortion. Different projections preserve different properties of the world, such as the relative shape of features, area, distance, or angle. For that reason, it’s important to pick a projection that would provide the highest accuracy for your region and the analysis you’re running. A common projection to use is the Universal Transverse Mercator or UTM. Figure 2.5: UTM around the globe Figure 2.6: UTM for the US If your study region is in Utah, for example, you would use UTM Zone 12 N (or UTM 12N). Note that while you will always have a datum, you do not necessarily need to ALWAYS use a projection. As for anything, it depends on your analysis and your system. Coordinate Reference System A coordinate reference system or CRS is simply the combination of the datum (geographic coordinate system) and the projection (projected coordinate system). For example, if you are working with the 1984 World Geodetic System that is projected to UTM Zone 12N, your CRS would be WGS84 UTM12N. If you are working with the 1983 North America Datum that is projected to UTM Zone 14N, your CRS would be NAD83 UTM14N. And so on. These different combinations of CRS all have their own EPSG code. (These codes were orginally created by the European Petroleum Survey Group, which is where the acronym comes from). For example, the EPSG code for WGS84 latitude/longitude (i.e. no projection) is 4326, the EPSG code for NAD83 UTM12N is 26912, and so on. These codes can easily be found on the Spatial Reference Website (or google if you forget what the website is). 2.2 Spatial Data Vectors Vector data are shapes with a geometry that can represent real world features. These geometries which can be made up of one or more vertices and paths. A vertex describes a position in space with x and y coordinates. A feature with one vertex would be a point, a feature with two or more vertices where the first and last vertices don’t connect would be a polyline, and a feature with at least three vertices and the first and last vertices connect (an enclosed area) would be polygon. Here are some examples of vector data that you might encounter in ecology: Points animal positional locations study site coordinates tree locations Lines roads fences boundaries rivers Areas (or polygons) bodies of water parks USFS land study plots area burned by a fire Example with random vertices: Figure 2.7: Figure demonstrating points (red), polylines (black), and polygon (blue) Or an example with Utah features: Figure 2.8: Figure demonstrating points (major Utah cities), polylines (major Utah highways), and polygons (shape of Utah boundary) Vector features have attributes, which can be text or numerical information that describe the features. These attributes are stored in a data frame. ## name state pop lat long capital ## 1 Bountiful UT UT 41622 40.88 -111.87 0 ## 2 Layton UT UT 63096 41.08 -111.95 0 ## 3 Logan UT UT 45262 41.74 -111.84 0 ## 4 Murray UT UT 56848 40.65 -111.89 0 ## 5 Ogden UT UT 78572 41.23 -111.97 0 ## 6 Orem UT UT 94758 40.30 -111.70 0 ## 7 Provo UT UT 105832 40.25 -111.64 0 ## 8 Saint George UT UT 63952 37.08 -113.58 0 ## 9 Salt Lake City UT UT 177318 40.78 -111.93 1 ## 10 Sandy UT UT 89698 40.57 -111.85 0 ## 11 Taylorsville UT UT 58200 40.66 -111.94 0 ## 12 West Jordan UT UT 105629 40.60 -111.99 0 ## 13 West Valley City UT UT 113989 40.69 -112.01 0 Some important information you need from your vector data are: the geometry type (if it’s a point, line, or polygon) the coordinate reference system the bounding box (the min/max points in x and y geographical space) Rasters Rasters are data represented by pixels (or cells) where each pixel has its own value. These cell values can be continuous (e.g. elevation, temperature, snow depth) or discrete (e.g. land cover, habitat type, presence/absence). Figure 2.9: Map showing a raster with continuous values (elevation) Figure 2.10: Map showing a raster with discrete values (land cover) Raster data can have more than one band (where each band is a single raster). These raster layers can stack together to create a Raster Stack. For example, satellite imagery is a stack of 3 rasters, each containing continuous values indicating levels of Red, Green, and Blue. ## class : SpatRaster ## dimensions : 1679, 2312, 3 (nrow, ncol, nlyr) ## resolution : 0.0008983153, 0.0008983153 (x, y) ## extent : -113.2237, -111.1468, 40.27956, 41.78783 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## source : landsat_demo.tif ## names : Red, Green, Blue Figure 2.11: Plotting a single band of a satellite image will only show the individual values of RGB But these 3 bands come together to make a true-color image. Figure 2.12: True-color satellite image of the Salt Lake region The important information you need from your raster data are the coordinate reference system the extent (the min/max points in x and y geographical space) the cell resolution (the width and height of each cell) The cell resolution basically means how “pixel-y” the raster is. A finer resolution (meaning the cell size is smaller) will have more detail than a coarser resolution (meaning the cell size is larger). For example, compare a raster with a pretty fine resolution (in this case 30m X 30m, meaning that each cells is 30-m wide and 30-m high) Figure 2.13: Map showing a raster with fine resolution (30m X 30m) Compared to the same raster but with a coarser resolution (in this case 300 m X 300 m) Figure 2.14: Map showing a raster with coarse resolution (300 m X 300 m) Wouldn’t we always want to work with finer resolutions? If rasters with finer resolutions have more detail (and thus are more accurate to what’s actually on the landscape) than one with a coarser resolution, why would we ever work with a raster with coarse resolution? I can think of 2 reasons why: Sometimes you simply can’t obtain that data in a finer resolution. For example, MODIS offers NDVI rasters every 16 days, but the finest resolution is 250-m. The finer the resolution, the more cells there are, and so the time, computation power, and disk space to do any sort of computation or analysis on these cells increases. As for everything, it depends on your analysis and your system. 2.3 Vectors vs Rasters: pros &amp; cons Advantages of Vectors Because vectors are just vertices and paths (rather than upwards of millions of grid cells), it takes less time to load, save, or perform any computation or analysis on a vector compared to a raster. (They also take up less disk space on your hard drive) For the same reason, they can often be more geographically accurate. A vector’s vertex is located at a single lat/long coordinate compared to a raster pixel at the same location but covers 250mX250m. Disadvantages of Vectors It is difficult to store and display continuous data in vectors. (It can be done, but the data typically would need to be binned) Vectors are best used to represent features of the landscape, rather than the landscape itself. Advantages of Rasters Rasters are best for satellite and other remotely sensed data. As the point above mentioned, they are great for representing the landscape itself. It is relatively easy and intuitive to perform any quantitative analysis with rasters. When raster cells are stacked (see figure below), it is pretty straightforward to perform any focal statistics or cell algebra. Figure 2.15: A stack of rasters, showing how each cell would correspond to the ones on top and below Disadvantages of Rasters Depending on the resolution, they can look pixellated and not visually appealing. For analysis, this would affect computation time and disk space. Raster cells can only contain one value (compared to vectors, which can have an entire attribute table). If you want cells to contain more than one value, you would need a stack of rasters, which takes up disk space and computation power. Now that we know about coordinate reference systems, vectors, and rasters, let’s learn how to deal with all of these in R! "],["spatial-data-in-r.html", "Chapter 3 Spatial Data in R 3.1 Vectors 3.2 Rasters", " Chapter 3 Spatial Data in R 3.1 Vectors The primary packages that deal with these features are sf and sp sf means “simple feature” while sp is short for “spatial.” This website goes into more detail about these packages and the specifics about objects from these packages. Please note, however, that sp is being deprecated soon and R users are encouraged to solely use sf. So while sp functions still work for now, we’re going to consider it deprecated and just go over sf in this workshop. 3.1.1 Loading vector data from a spreadsheet You will often have a spreadsheet of data that have two columns of latitude and longitude (and potentially other columns for the attributes of these features). sites &lt;- read.csv(&quot;Data/Examples/Sites.csv&quot;) head(sites) ## Site Latitude Longitude ## 1 1 38.94893 -110.9233 ## 2 2 40.40730 -112.5923 ## 3 3 40.58137 -110.1357 ## 4 4 41.04609 -113.3415 ## 5 5 40.78323 -110.8535 ## 6 6 41.07762 -112.0717 class(sites) ## [1] &quot;data.frame&quot; How can we convert this data table into a spatial object? We’ll use the function st_as_sf() to convert an object of another class to an sf object. As with any function, you can always type ?st_as_sf in the console to remind yourself what the function does and what arguments you need to add. In this case, the non-optional arguments are x (the object we want to convert, in this case our sites data frame) and coords (the columns in the data frame that have the coordinate data). The argument crs is optional but we should add it so that our spatial object has a coordinate reference system. Because our coordinates are lat/long, we should tell st_as_sf that our crs is WGS84 lat/long (Remember that the EPSG code for WGS84 lat/long is 4326). sites_sf &lt;- st_as_sf(sites, coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), crs = 4326) head(sites_sf) ## Simple feature collection with 6 features and 1 field ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -113.3415 ymin: 38.94893 xmax: -110.1357 ymax: 41.07762 ## Geodetic CRS: WGS 84 ## Site geometry ## 1 1 POINT (-110.9233 38.94893) ## 2 2 POINT (-112.5923 40.4073) ## 3 3 POINT (-110.1357 40.58137) ## 4 4 POINT (-113.3415 41.04609) ## 5 5 POINT (-110.8535 40.78323) ## 6 6 POINT (-112.0717 41.07762) Now when we look at the head of sites_sf it also prints out some information about this spatial object: it is a simple feature, it is a POINT object, the min/max coordinates (the bounding box), and the CRS. Note that the “Longitude” and “Latitude” columns turned into a “geometry” column. We can also check this information with separate functions: st_bbox and st_crs st_bbox(sites_sf) ## xmin ymin xmax ymax ## -113.79094 37.09849 -109.43727 41.08479 st_crs(sites_sf) ## Coordinate Reference System: ## User input: EPSG:4326 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## ENSEMBLE[&quot;World Geodetic System 1984 ensemble&quot;, ## MEMBER[&quot;World Geodetic System 1984 (Transit)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G730)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G873)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1150)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1674)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1762)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G2139)&quot;], ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]], ## ENSEMBLEACCURACY[2.0]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;geodetic latitude (Lat)&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;geodetic longitude (Lon)&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## USAGE[ ## SCOPE[&quot;Horizontal component of 3D system.&quot;], ## AREA[&quot;World.&quot;], ## BBOX[-90,-180,90,180]], ## ID[&quot;EPSG&quot;,4326]] st_crs printed out a lot of information, but the important things to note are that we (the user) inputted EPSG: 4326, its datum is “World Geodetic System 1984” (or WGS 84), its axes are latitude and longitude, and its units are “degree” (as opposed to a linear unit like m that we might see in a projected crs). It’s always good practice to double check the CRS of your object before you perform any analysis. Note that now when we look at the class of sites_sf it has two classes: sf and data.frame. class(sites_sf) ## [1] &quot;sf&quot; &quot;data.frame&quot; This is one of the great things about sf: because sf objects remain as class data.frame, it makes it easy to work with an sf object like you would with any data frame. Now that it’s a spatial object, let’s plot it and see what it looks like! plot(sites_sf, pch = 16) This object only has one attribute (“Site”). If it had more than one, plotting in base R (i.e. using the plot function) plots all attributes (it will actually only plot 10 and will give you a warning if there are more than 10 attributes). To just plot the feature’s shape, we can use st_geometry to extract the geometry of the vector plot(st_geometry(sites_sf), pch = 16) We can also use ggplot2 to plot sf objects. ggplot2 has a function called geom_sf, which makes it easy to visualize sf objects with ggplot2 ggplot(sites_sf) + geom_sf() # note that we don&#39;t need to tell ggplot what it should consider x and y to be; that&#39;s already built in with the &quot;geometry&quot; column! Remember, though, that this workshop will not cover how to make nice maps in R (that will be in the next workshop). I just bring this up because you would want to be sure the data you’re working with looks like what you might expect it to, so you would want to make a quick plot before moving on to analysis. 3.1.2 Loading vector data from a shapefile A lot of the time, you will have spatial data already saved as a shapefile. How do we load that into R? We’ll use the function st_read which takes two arguments: dsn (data source name), which is essentially the folder where the shapefile is located, and layer, which is the name of file (without any extensions). layer is technically optional because dsn will choose the first file in that folder, so if the shapefile is the only file in that folder, then dsn will automatically choose the file. But I like to specify the layer name to avoid any mishaps. # Shapefile: Interstates in Utah interstate_sf &lt;- st_read(dsn = &quot;Data/Examples/utah_interstate&quot;, layer = &quot;utah_interstate&quot;) ## Reading layer `utah_interstate&#39; from data source ## `C:\\Users\\RonanHart\\Documents\\Projects\\R_Spatial_Visualization_Workshop\\Data\\Examples\\utah_interstate&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 1848 features and 9 fields ## Geometry type: LINESTRING ## Dimension: XY ## Bounding box: xmin: -114.0437 ymin: 37.00002 xmax: -109.0513 ymax: 42.00117 ## Geodetic CRS: WGS 84 You’ll note that as we read in a shapefile with st_read, it automatically prints out the information about this feature (if you don’t want R to print this, put quiet = FALSE in the function). Again, we can check this information separately (in case we forget or we need to check later in the analysis). We can also check the first few rows of the data frame to see what kind of attributes there are, and we can plot the feature to make sure it looks like what we expect it to be. st_bbox(interstate_sf) ## xmin ymin xmax ymax ## -114.04367 37.00002 -109.05128 42.00117 st_crs(interstate_sf) ## Coordinate Reference System: ## User input: WGS 84 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;latitude&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;longitude&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4326]] head(interstate_sf) ## Simple feature collection with 6 features and 9 fields ## Geometry type: LINESTRING ## Dimension: XY ## Bounding box: xmin: -112.9847 ymin: 38.5741 xmax: -111.8053 ymax: 40.83991 ## Geodetic CRS: WGS 84 ## OBJECTID FULLNAME NAME POSTTYPE SPEED_LMT DOT_HWYNAM DOT_SRFTYP ## 1 523 I-15 NB FWY I-15 NB FWY 75 I-15 P ## 2 649 I-215W SB FWY I-215W SB FWY 65 I-215 P ## 3 857 I-80 WB FWY I-80 WB FWY 75 I-80 P ## 4 1006 I-215W NB FWY I-215W NB FWY 65 I-215 P ## 5 1032 I-15 NB FWY I-15 NB FWY 75 I-15 P ## 6 1106 I-215E SB FWY I-215E SB FWY 65 I-215 P ## DOT_AADT UNIQUE_ID geometry ## 1 21000 12SUH60347079_I-15 NB_FWY LINESTRING (-112.6033 38.57... ## 2 NA 12TVL19841035_I-215W SB_FWY LINESTRING (-111.9491 40.74... ## 3 NA 12TUL32701512_I-80 WB_FWY LINESTRING (-112.9847 40.76... ## 4 37000 12TVL22892135_I-215W NB_FWY LINESTRING (-111.9145 40.83... ## 5 22000 12SUJ79150753_I-15 NB_FWY LINESTRING (-112.4074 38.89... ## 6 68000 12TVL31740094_I-215E SB_FWY LINESTRING (-111.8053 40.67... ggplot(interstate_sf) + geom_sf() 3.1.3 Projecting vector data So far all of our vector data has only been in WGS 84 lat/long CRS. More than likely you will want to work in a projected coordinate system. So how do we re-project (or transform) features in R? We will use the sf function st_transform() This function needs the object you want to re-project and the target CRS. What CRS should we work in? As was stated in the “GIS basics” chapter, UTM is a popular projection because it’s localized and allows you to work with with linear metrics like length, area, and distance. Since both of these features (our site data and Utah interstates) are located in Utah, we would use UTM 12N. We can stay in the WGS 84 datum, but for the purposes of demonstration, let’s change to NAD83. (Remember from the previous chapter that there is not much difference between these datums, so it’s up to you if you want to use a more localized datum (NAD83) or a more recent datum (WGS84)). The EPSG code for NAD83 UTM 12N is 26912. (If you want to work in WGS84 UTM 12N, the EPSG code is 32612) interstate_sf_proj &lt;- st_transform(interstate_sf, crs = 26912) st_crs(interstate_sf_proj) ## Coordinate Reference System: ## User input: EPSG:26912 ## wkt: ## PROJCRS[&quot;NAD83 / UTM zone 12N&quot;, ## BASEGEOGCRS[&quot;NAD83&quot;, ## DATUM[&quot;North American Datum 1983&quot;, ## ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4269]], ## CONVERSION[&quot;UTM zone 12N&quot;, ## METHOD[&quot;Transverse Mercator&quot;, ## ID[&quot;EPSG&quot;,9807]], ## PARAMETER[&quot;Latitude of natural origin&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8801]], ## PARAMETER[&quot;Longitude of natural origin&quot;,-111, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8802]], ## PARAMETER[&quot;Scale factor at natural origin&quot;,0.9996, ## SCALEUNIT[&quot;unity&quot;,1], ## ID[&quot;EPSG&quot;,8805]], ## PARAMETER[&quot;False easting&quot;,500000, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8806]], ## PARAMETER[&quot;False northing&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8807]]], ## CS[Cartesian,2], ## AXIS[&quot;(E)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;metre&quot;,1]], ## AXIS[&quot;(N)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;metre&quot;,1]], ## USAGE[ ## SCOPE[&quot;Engineering survey, topographic mapping.&quot;], ## AREA[&quot;North America - between 114°W and 108°W - onshore and offshore. Canada - Alberta; Northwest Territories; Nunavut; Saskatchewan. United States (USA) - Arizona; Colorado; Idaho; Montana; New Mexico; Utah; Wyoming.&quot;], ## BBOX[31.33,-114,84,-108]], ## ID[&quot;EPSG&quot;,26912]] Note that the coordinates of the WGS84 object compared the coordinates of the projected object are different. st_geometry(interstate_sf) ## Geometry set for 1848 features ## Geometry type: LINESTRING ## Dimension: XY ## Bounding box: xmin: -114.0437 ymin: 37.00002 xmax: -109.0513 ymax: 42.00117 ## Geodetic CRS: WGS 84 ## First 5 geometries: st_geometry(interstate_sf_proj) ## Geometry set for 1848 features ## Geometry type: LINESTRING ## Dimension: XY ## Bounding box: xmin: 242993.7 ymin: 4098090 xmax: 668288.9 ymax: 4651339 ## Projected CRS: NAD83 / UTM zone 12N ## First 5 geometries: The coordinates for the object in WGS84 are in lat/long and the units are decimal degrees. The coordiantes for the projected object are in UTM and the untis are meters. REMINDER: You should always double-check the CRS of every feature and object you’re working with because you want to make sure everything is projected to the same CRS! If they’re not, it could result in inaccurate analysis and plotting. Depending on what functions you’re using, R may give you a warning or error that the CRS of one feature and another don’t match, but it’s best not to rely on this and just double-check yourself. 3.1.4 Saving vector data Once you’ve created or modified a spatial object, most likely you want to save it so you can use it later or share with collaborators. There are many files types that can hold spatial data, but the most commonly used are ESRI Shapefile (which can be used in ArcGIS software), KML (which can be used with Google Earth), GeoJSON, and PostgreSQL. If you’re curious what other file types are out there, you can use function st_drivers() for sf objects. In this workshop, I will focus just on ESRI Shapefiles. If you’ve worked with shapefiles before, you’ve likely noticed that a shapefile is actually a collection of files with the extensions .dbf, .prj, .shp, and .shx. It’s often easier to organize and share shapefiles if they’re in their own folder. If you don’t already have a folder ready for any shapefiles that you’re ready to save, you can, of course, manually make one in the File Explorer. But you can also create folders in R! I, personally, often like to do this because it helps streamline my process and keeps the workflow reproducible. To do so we’ll use the functions dir.exists() to check if the directory (or folder) exists or not, and if it doesn’t we’ll use dir.create() to create a directory. out_dir &lt;- &quot;Data/Examples/Sites_shp&quot; # name what you want the folder to be and save it in an object if(!dir.exists(out_dir)){ # this if statement is basically saying &quot;if out_dir does NOT exist...&quot; (the NOT is from the exclamation mark) dir.create(out_dir) # and if out_dir does NOT exist, then dir.create will create it } Now that we’ve created a new directory for this shapefile to go to, let’s save our shapefile! For sf objects we’ll use the function st_write. This function takes the object you’re saving, the dsn (or the folder) you want to save it to, the layer (or the name you want to save the file as, do NOT add an extension), and the driver (meaning if you’re saving it as an ESRI Shapefile, KML, etc.) st_write(sites_sf, dsn = out_dir, layer = &quot;Sites_sf&quot;, driver = &quot;ESRI Shapefile&quot;) A note about sf and tidyverse As mentioned earlier, sf works really well with tidyverse. If you are familiar with tidyverse then you know that one pro of this package is the pipe (%&gt;%) which lets you perform multiple functions at once without getting cluttered and hard to read. Functions for sf can easily be incorporated into the tidyverse piping method as well. For an example, we can load a csv file, convert it to an sf object, project it, and save it all at once using the pipes. read.csv(&quot;Data/Examples/Sites.csv&quot;) %&gt;% st_as_sf(coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), crs = 4326) %&gt;% st_transform(crs = 26912) %&gt;% st_write(dsn = &quot;data&quot;, layer = &quot;Sites_shp&quot;, driver = &quot;ESRI Shapefile&quot;) Of course, this requires that you already know what the column names for the coordinates are, and this might not be the best example for a series of functions to use with the pipe. But this type of process is a useful tool to know, especially if you are already familiar with and frequently use tidyverse. Your Turn! In the attached “worksheet”, there are some exerices to help you practice these concepts and functions. Try them out! 3.2 Rasters The primary package that deals with rasters is terra. raster also works with rasters but, just like sp, will be deprecated soon, so in this workshop we will work with terra. (Note that raster is functionally very similar to terra, so it is easy to switch between the two. But while raster is still functional, it will no longer be updated). 3.2.1 Load a raster When you want to load a raster that is saved in a directory, you’ll use the function rast. When loading a raster that is already saved, the only argument you need is the filename (including the directory and extension) elev &lt;- rast(&quot;Data/Examples/elevation.tif&quot;) elev ## class : SpatRaster ## dimensions : 334, 390, 1 (nrow, ncol, nlyr) ## resolution : 0.000278, 0.000278 (x, y) ## extent : -114.0275, -113.9191, 36.40381, 36.49667 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## source : elevation.tif ## name : elevation ## min value : 645.7881 ## max value : 1137.6703 When we examine the raster (by simply calling the object like we did above), we get a lot of useful information. class: the class of the raster (this could be RasterLayer, RasterStack, or RasterBrick) dimensions: the number of rows, columns, and cells resolution: the size of the cells extent: the min/max x and y crs: the coordinate reference system values: the min/max values this raster contains I recommend always examining a raster after you load it in to make sure the information looks like what you would expect it to be. You can also check all of this information with separate functions: class(elev) ## [1] &quot;SpatRaster&quot; ## attr(,&quot;package&quot;) ## [1] &quot;terra&quot; nrow(elev) ## [1] 334 ncol(elev) ## [1] 390 ncell(elev) ## [1] 130260 res(elev) # the cell resolution of the raster ## [1] 0.000278 0.000278 ext(elev) ## SpatExtent : -114.027500001, -113.919080001, 36.403814658, 36.496666658 (xmin, xmax, ymin, ymax) cat(crs(elev)) # I put cat() around crs() because it will otherwise print out a full string full of &quot;\\n&quot; (which tell R to print on a new line, so using cat() will make this output more readable) ## GEOGCRS[&quot;WGS 84&quot;, ## ENSEMBLE[&quot;World Geodetic System 1984 ensemble&quot;, ## MEMBER[&quot;World Geodetic System 1984 (Transit)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G730)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G873)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1150)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1674)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1762)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G2139)&quot;], ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]], ## ENSEMBLEACCURACY[2.0]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;geodetic latitude (Lat)&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;geodetic longitude (Lon)&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## USAGE[ ## SCOPE[&quot;Horizontal component of 3D system.&quot;], ## AREA[&quot;World.&quot;], ## BBOX[-90,-180,90,180]], ## ID[&quot;EPSG&quot;,4326]] summary(values(elev)) # put this in summary() so we can get an idea of the spread of values (rather than a list of all of the values themselves) ## elevation ## Min. : 645.8 ## 1st Qu.: 798.5 ## Median : 869.5 ## Mean : 864.4 ## 3rd Qu.: 932.0 ## Max. :1137.7 We can also plot the raster to check that it looks like what we would expect it to be plot(elev) We can also plot a raster with ggplot2, but we need to do an extra step of converting it to a data frame first. We’ll use the base R function as.data.frame and we need to be sure to put xy = TRUE in the arguments. After we do that, we can use the function geom_raster() within ggplot. Within the aes() function in geom_raster(), we need to put the columns that correspond to the x and y locations and what the fill value should represent (in this case, elevation). elev_df &lt;- as.data.frame(elev, xy = TRUE) head(elev_df) # the data frame of a raster contains columns for the x and y locations and the corresponding cell value ## x y elevation ## 1 -114.0274 36.49653 1082.456 ## 2 -114.0271 36.49653 1081.739 ## 3 -114.0268 36.49653 1080.975 ## 4 -114.0265 36.49653 1079.712 ## 5 -114.0262 36.49653 1077.750 ## 6 -114.0260 36.49653 1075.466 ggplot(elev_df) + geom_raster(aes(x = x, y = y, fill = elevation)) # You can use the pipe to run this process all at once elev %&gt;% as.data.frame(xy = TRUE) %&gt;% ggplot() + geom_raster(aes(x = x, y = y, fill = elevation)) # This requires that you know the column name of the raster values An important thing to note is that, depending on the size of the raster, it might be too computationally expensive to convert a raster to a data frame and R may give you an error. (Another reminder that we will not cover how to make these maps look nice, this is just a quick way to check that your raster looks the way that you think it should) Loading a raster with more than one band In the example above, the raster we loaded only had one band. Let’s try loading in a raster downloaded from SNODAS (Snow Data Assimilation System) which has 2 bands: SWE (snow water equivalent) and snow depth. (SNODAS offers daily rasters, and this example raster is from 02-23-2019). terra makes it easy to load a stacked raster because we can just use the same function rast. snow_stack &lt;- rast(&quot;Data/Examples/snow_20190223.tif&quot;) snow_stack ## class : SpatRaster ## dimensions : 12, 13, 2 (nrow, ncol, nlyr) ## resolution : 0.008333333, 0.008333333 (x, y) ## extent : -114.025, -113.9167, 36.4, 36.5 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## source : snow_20190223.tif ## names : snow_20190223_1, snow_20190223_2 ## min values : 0, 0 ## max values : 24, 150 Let’s plot the raster stack just to make sure it looks like what we might expect: plot(snow_stack) Note that plotting a raster stack will plot all the layers at once. The benefit of working with a stack of rasters is that if you need to perform a computation on all of your rasters, you can stack them all togther and the computation will run for every raster layer in the stack. To stack rasters together we use the function c(), similar to how we would add elements to vector. However, rasters will only stack together if they have the same dimensions, resolution, extent, and crs. For example, if we try to stack our elevation raster and stack of snow rasters together: c(elev, snow_stack) Error: [rast] extents do not match We get an error that the extents don’t match ext(elev) ## SpatExtent : -114.027500001, -113.919080001, 36.403814658, 36.496666658 (xmin, xmax, ymin, ymax) ext(snow_stack) ## SpatExtent : -114.025, -113.916666666667, 36.3999999996667, 36.4999999996667 (xmin, xmax, ymin, ymax) They’re very close, but they need to be exact. Why can’t I just not stack them and run seperate computations on these rasters? You can, but your raster cells will be slightly off from each other and may result in inaccurate results and inference. Figure 3.1: A stack of rasters, showing how each cell would correspond to the ones on top and below In a future section, we’ll talk about how to make your rasters line up so you can stack them all together. 3.2.2 Create a raster To create a raster from scratch, we’ll use the same function rast() but instead of the filename or band as arguments, we’ll include the extent and resolution that we want. In this example, let’s just make something random. set.seed(1) # this is to make the randomness of runif the same every time x &lt;- runif(1, min = 0, max = 10) y &lt;- runif(1, min = 0, max = 10) r_new &lt;- rast(xmin = x, xmax = x + 10, ymin = y, ymax = y + 10, resolution = 1) r_new ## class : SpatRaster ## dimensions : 10, 10, 1 (nrow, ncol, nlyr) ## resolution : 1, 1 (x, y) ## extent : 2.655087, 12.65509, 3.721239, 13.72124 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 Note that because we didn’t specify the CRS in the arguments, it defaulted to WGS84 lat/long. Right now this raster is empty. We can check with the function values() head(values(r_new)) ## lyr.1 ## [1,] NaN ## [2,] NaN ## [3,] NaN ## [4,] NaN ## [5,] NaN ## [6,] NaN We’ll use the same function to assign fill the raster with values. If you noticed, there are 100 total cells that need to be filled, and let’s fill them with random numbers. values(r_new) &lt;- runif(100) head(values(r_new)) ## lyr.1 ## [1,] 0.5728534 ## [2,] 0.9082078 ## [3,] 0.2016819 ## [4,] 0.8983897 ## [5,] 0.9446753 ## [6,] 0.6607978 r_new ## class : SpatRaster ## dimensions : 10, 10, 1 (nrow, ncol, nlyr) ## resolution : 1, 1 (x, y) ## extent : 2.655087, 12.65509, 3.721239, 13.72124 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 ## source(s) : memory ## name : lyr.1 ## min value : 0.01339033 ## max value : 0.99190609 plot(r_new) Why would you need to create a raster from scratch? If you’ve written code to perform a calculation or an analysis on a raster, it’s good to check that your code is doing what you think it’s doing. A good way to test the code is to try it out on a “dummy” raster If you’re asking for help on your code (on Stack Overflow for example), it’s good practice to either include your data (which is often very difficult to do) or to create a reproducible example. Knowing how to create fake data or fake rasters is useful to know. We’ll see in the next section another useful reason for creating a raster from scratch. 3.2.3 Project a raster Let’s go back to the elevation raster we loaded earlier. If you noticed, the CRS of this raster is WGS84 UTM12N, as opposed to NAD83 UTM12N that we used earlier for our vector data. cat(crs(elev)) ## GEOGCRS[&quot;WGS 84&quot;, ## ENSEMBLE[&quot;World Geodetic System 1984 ensemble&quot;, ## MEMBER[&quot;World Geodetic System 1984 (Transit)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G730)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G873)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1150)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1674)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1762)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G2139)&quot;], ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]], ## ENSEMBLEACCURACY[2.0]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;geodetic latitude (Lat)&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;geodetic longitude (Lon)&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## USAGE[ ## SCOPE[&quot;Horizontal component of 3D system.&quot;], ## AREA[&quot;World.&quot;], ## BBOX[-90,-180,90,180]], ## ID[&quot;EPSG&quot;,4326]] If we were going to do some sort of analysis on our vector data and raster data, we want all of our spatial objects to be in the same CRS so that we know that our layers are truly stacked on top of the other (for plotting and analysis). For that reason, we need to project this raster to NAD83 UTM12N. To do so, we’ll use the function project elev_proj &lt;- project(x = elev, &quot;epsg:26912&quot;) elev_proj ## class : SpatRaster ## dimensions : 384, 364, 1 (nrow, ncol, nlyr) ## resolution : 27.59883, 27.59883 (x, y) ## extent : 228498.9, 238544.8, 4032704, 4043302 (xmin, xmax, ymin, ymax) ## coord. ref. : NAD83 / UTM zone 12N (EPSG:26912) ## source(s) : memory ## name : elevation ## min value : 645.913 ## max value : 1136.788 Wait, let’s take a look at the cell resolution again… res(elev_proj) ## [1] 27.59883 27.59883 This elevation raster came from the USGS’s 3D Elevation Program. When I downloaded it, I specified a 1-arcsecond resolution (arcsecond is the unit for long/lat projections), which is equivalent to a 30mX30m resolution in a linear projection (like UTM). So why is the resolution after projecting not actually 30mX30m? What’s happening is that every cell needs to be projected, causing the cells to be warped and results in slightly off cell measurements. What if we tried specifying the cell resolution in the projectRaster function? elev_proj_2 &lt;- project(elev, &quot;epsg:26912&quot;, res = 30) elev_proj_2 ## class : SpatRaster ## dimensions : 353, 335, 1 (nrow, ncol, nlyr) ## resolution : 30, 30 (x, y) ## extent : 228498.9, 238548.9, 4032704, 4043294 (xmin, xmax, ymin, ymax) ## coord. ref. : NAD83 / UTM zone 12N (EPSG:26912) ## source(s) : memory ## name : elevation ## min value : 645.8972 ## max value : 1136.5520 We could do this. But can we be sure that all of the cells in this raster would align one-to-one in all of our other rasters? And if they don’t align, can we be sure our analysis and computation on all of our rasters would be correct? snow_proj &lt;- project(snow_stack, &quot;epsg:26912&quot;, res = 30) snow_proj ## class : SpatRaster ## dimensions : 389, 334, 2 (nrow, ncol, nlyr) ## resolution : 30, 30 (x, y) ## extent : 228709.8, 238729.8, 4031990, 4043660 (xmin, xmax, ymin, ymax) ## coord. ref. : NAD83 / UTM zone 12N (EPSG:26912) ## source(s) : memory ## names : snow_20190223_1, snow_20190223_2 ## min values : 0.00000, 0.0000 ## max values : 23.99983, 149.8608 c(elev_proj_2, snow_proj) Error: [rast] extents do not match Furthermore, there’s something else happening when rasters are projected. Let’s double-check the help file, ?project. It looks like there’s another argument called method that defaults to \"bilinear\". Ok…well what does that mean? Bilinear interpretation is basically a way of estimating a value in between two other values. This essentially means that when you use project, the raster cells are being warped and shifted slightly, so R can’t just simply move the entire raster over, but it needs to know how to estimate the cell values in their new location and new resolution. So should it estimate a value in between two cell values or pick the actual value of the cell it’s closest to? Either way, this will slightly adjust the data: elev_bilinear &lt;- project(elev, &quot;epsg:26912&quot;, res = 30, method = &quot;bilinear&quot;) elev_sum &lt;- project(elev, &quot;epsg:26912&quot;, res = 30, method = &quot;sum&quot;) # check the spread of values in the orginal raster summary(values(elev)) ## elevation ## Min. : 645.8 ## 1st Qu.: 798.5 ## Median : 869.5 ## Mean : 864.4 ## 3rd Qu.: 932.0 ## Max. :1137.7 # check the spread of values in the raster estimated with bilinear summary(values(elev_bilinear)) ## elevation ## Min. : 645.9 ## 1st Qu.: 798.5 ## Median : 869.5 ## Mean : 864.4 ## 3rd Qu.: 932.1 ## Max. :1136.6 ## NA&#39;s :6873 # check the spread of values in the raster estimated with nearest neighbor summary(values(elev_sum)) ## elevation ## Min. : 0.001 ## 1st Qu.: 930.229 ## Median :1015.933 ## Mean :1004.808 ## 3rd Qu.:1089.200 ## Max. :1329.881 ## NA&#39;s :6197 There’s no right or wrong answer on which method you pick. The project help file recommends using bilinear for continuous rasters (like this elevation raster we’re working with) and sum (or min, q1, med, q3, max, average, mode, or rms) for categorical rasters (in fact, you shouldn’t use bilinear for categorical rasters because it will pick a value that’s in between two values which aren’t real numbers, but are actually stand-ins for a category). In my work I use sum for continuous rasters as well, but bilinear is valid to use as well. (Again, as with everything, it depends on your analysis and your system.) We’ll continue to use bilinear in this workshop. One thing to keep in mind with projecting rasters is that you should not over-project and re-sample your rasters over and over again. Imagine printing a picture and then making a copy of that printed picture. And then making a copy of the copy. And then a copy of that that copy. Over and over again. Eventually you end up with a warped image that only vaguely resembles the original image. A similar thing can happen if you project and re-sample your rasters even more than once. (FYI there is no issue with projecting and re-projecting your vector data over and over again. This is because vector data are made up of lines and vertices which are relatively easy to project, and the data within a vector stays the same while it’s being projected.) Ok, so now that we know the dangers of projecting rasters, what is the best way project a raster? I recommend (and projectRaster also recommends) creating a template raster and using that template as a mold for all of your rasters. (hint: this is where it comes in handy to know how to make a blank raster) So let’s create a template raster with our desired CRS, cell resolution, and extent. To choose the cell resolution, I recommend picking your set of rasters with the smallest resolution (but keep in mind that this will affect disk space and computation time, so again there’s no right or wrong answer). In this case, elevation (30mX30m) has a smaller resolution than snow (1kmX1km). The extent should be the min and max area of focus, such as your study area. # create a blank raster with desired parameters. We don&#39;t need to add any values to it template &lt;- rast(crs = &quot;epsg:26912&quot;, resolution = c(30, 30), xmin = 228560, xmax = 238910, ymin = 4032145, ymax = 4043815) template ## class : SpatRaster ## dimensions : 389, 345, 1 (nrow, ncol, nlyr) ## resolution : 30, 30 (x, y) ## extent : 228560, 238910, 4032145, 4043815 (xmin, xmax, ymin, ymax) ## coord. ref. : NAD83 / UTM zone 12N (EPSG:26912) # use the template to project our rasters # from = focal raster # to = the raster object with the desired parameters # method = the method to compute new values elev_proj_template &lt;- project(elev, template) snow_proj_template &lt;- project(snow_stack, template) elev_proj_template ## class : SpatRaster ## dimensions : 389, 345, 1 (nrow, ncol, nlyr) ## resolution : 30, 30 (x, y) ## extent : 228560, 238910, 4032145, 4043815 (xmin, xmax, ymin, ymax) ## coord. ref. : NAD83 / UTM zone 12N (EPSG:26912) ## source(s) : memory ## name : elevation ## min value : 645.8929 ## max value : 1136.4094 snow_proj_template ## class : SpatRaster ## dimensions : 389, 345, 2 (nrow, ncol, nlyr) ## resolution : 30, 30 (x, y) ## extent : 228560, 238910, 4032145, 4043815 (xmin, xmax, ymin, ymax) ## coord. ref. : NAD83 / UTM zone 12N (EPSG:26912) ## source(s) : memory ## names : snow_20190223_1, snow_20190223_2 ## min values : 0.00000, 0.0000 ## max values : 23.99982, 149.8712 # now we can stack them! This way we know for sure that all cells align with every layer stack &lt;- c(elev_proj_template, snow_proj_template) stack ## class : SpatRaster ## dimensions : 389, 345, 3 (nrow, ncol, nlyr) ## resolution : 30, 30 (x, y) ## extent : 228560, 238910, 4032145, 4043815 (xmin, xmax, ymin, ymax) ## coord. ref. : NAD83 / UTM zone 12N (EPSG:26912) ## source(s) : memory ## names : elevation, snow_20190223_1, snow_20190223_2 ## min values : 645.8929, 0.00000, 0.0000 ## max values : 1136.4094, 23.99982, 149.8712 plot(stack) Right now the bands’ names (besides “elevation”) are not very descriptive, but we can change their names so that we can more easily remember which band is which. In this case, band 1 is elevation, band 2 is SWE, and band 3 is snow depth. To change the band names, we’ll use the function names() names(stack) &lt;- c(&quot;elevation&quot;, &quot;swe&quot;, &quot;snow_depth&quot;) stack ## class : SpatRaster ## dimensions : 389, 345, 3 (nrow, ncol, nlyr) ## resolution : 30, 30 (x, y) ## extent : 228560, 238910, 4032145, 4043815 (xmin, xmax, ymin, ymax) ## coord. ref. : NAD83 / UTM zone 12N (EPSG:26912) ## source(s) : memory ## names : elevation, swe, snow_depth ## min values : 645.8929, 0.00000, 0.0000 ## max values : 1136.4094, 23.99982, 149.8712 3.2.4 Save a raster To save a raster we’ll use the function writeRaster. We need to include the raster object that we’re saving, the full file name (including the directory). We can also specify the file type with filetype, but this is often not necessary because writeRaster can guess based on the file extension you provide in filename (e.g., if you save a raster as “filename.tif”, writeRaster automatically guesses that you want to save this raster as a GeoTIFF) (If you need to save your raster as something besides a GeoTIFF, you can check the different file types by checking the help document for this function by typing ?writeRaster in the console and scrolling down to filetype.) writeRaster(stack, &quot;Data/Examples/elev_snow_stack.tif&quot;) Note that some raster functions have optional arguments to add filenames and formats so that the funciton will automatically save the output after performing its function. Your Turn! In the attached “worksheet”, there are some exercises to help you practice these concepts and functions. Try them out! Now that we know the basics of working with vector and raster data in R, let’s learn how to manipulate and perform computations on these spatial objects. "],["spatial-analysis.html", "Chapter 4 Spatial Analysis 4.1 Selecting Attributes 4.2 Select features by location 4.3 Joining Attributes 4.4 Extract Raster Values 4.5 Distance 4.6 Calculate Terrain Characteristics 4.7 Re-Classify Rasters 4.8 Raster Cell Stats 4.9 A Note About Loops", " Chapter 4 Spatial Analysis Now let’s do some analysis with the data we’ve acquired already: Sites point data sites_sf Utah interstates interstate_sf_proj We also have some data that I’ve included in the exercises portion of the “worksheet”: a different elevation + snow raster stack (this one is in the NW corner of Utah), elev_snow_stk ## class : SpatRaster ## dimensions : 240, 240, 3 (nrow, ncol, nlyr) ## resolution : 0.008333333, 0.008333333 (x, y) ## extent : -112, -110, 40, 42 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## source : elev_snow_nw_stack.tif ## names : elevation, swe, snow_depth ## min values : 1280.021, 0, 0 ## max values : 4087.231, 1108, 3196 plot(elev_snow_stk) a set of plots as a point feature, head(plots_sf) ## Simple feature collection with 6 features and 1 field ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -111.6469 ymin: 40.40336 xmax: -110.4603 ymax: 41.81642 ## Geodetic CRS: WGS 84 ## Plots geometry ## 1 A POINT (-111.5881 40.53102) ## 2 B POINT (-111.6469 40.74425) ## 3 C POINT (-110.626 41.14571) ## 4 D POINT (-111.2318 41.81642) ## 5 E POINT (-110.4603 40.40336) ## 6 F POINT (-111.0046 41.79678) and a polygon feature of land management boundaries in Utah ownership_sf &lt;- st_read(&quot;Data/Exercises/UT_land_ownership&quot;, quiet = T) %&gt;% st_transform(crs = 26912) ## Simple feature collection with 6 features and 5 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 388066 ymin: 4410117 xmax: 403492.2 ymax: 4414856 ## Projected CRS: NAD83 / UTM zone 12N ## OBJECTID OWNER AGENCY ADMIN DESIG geometry ## 1 2974453 Federal BLM BLM Bankhead Jones MULTIPOLYGON (((388854 4411... ## 2 2974454 Federal BLM BLM Bankhead Jones MULTIPOLYGON (((394528.1 44... ## 3 2974455 Federal BLM BLM Bankhead Jones MULTIPOLYGON (((388473.6 44... ## 4 2974456 Federal BLM BLM Bankhead Jones MULTIPOLYGON (((399793.3 44... ## 5 2974457 Federal BLM BLM Bankhead Jones MULTIPOLYGON (((389300.1 44... ## 6 2974458 Federal BLM BLM Bankhead Jones MULTIPOLYGON (((403492.2 44... Let’s plot one of the rasters with our sites point vector and Utah highways line vector. To plot just one raster layer in a stack we can either index it with double brackets or with the name: # these are different ways to get the same raster layer elev_snow_stk[[1]] ## class : SpatRaster ## dimensions : 240, 240, 1 (nrow, ncol, nlyr) ## resolution : 0.008333333, 0.008333333 (x, y) ## extent : -112, -110, 40, 42 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## source : elev_snow_nw_stack.tif ## name : elevation ## min value : 1280.021 ## max value : 4087.231 elev_snow_stk$elevation ## class : SpatRaster ## dimensions : 240, 240, 1 (nrow, ncol, nlyr) ## resolution : 0.008333333, 0.008333333 (x, y) ## extent : -112, -110, 40, 42 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## source : elev_snow_nw_stack.tif ## name : elevation ## min value : 1280.021 ## max value : 4087.231 ## Reading layer `utah_interstate&#39; from data source ## `C:\\Users\\RonanHart\\Documents\\Projects\\R_Spatial_Visualization_Workshop\\Data\\Examples\\utah_interstate&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 1848 features and 9 fields ## Geometry type: LINESTRING ## Dimension: XY ## Bounding box: xmin: -114.0437 ymin: 37.00002 xmax: -109.0513 ymax: 42.00117 ## Geodetic CRS: WGS 84 plot(elev_snow_stk$elevation) plot(st_geometry(interstate_sf), lwd = 2, add = TRUE) # add = TRUE will add other elements to the plot without erasing previous elements and creating a new plot plot(st_geometry(sites_sf), pch = 16, add = TRUE) plot(st_geometry(plots_sf), pch = 3, add = TRUE) (This can also be done with ggplot using as.data.frame) elev_snow_stk$elevation %&gt;% as.data.frame(xy = T) %&gt;% ggplot() + geom_raster(aes(x = x, y = y, fill = elevation)) + scale_fill_gradientn(colors = rev(terrain.colors(10))) + geom_sf(data = interstate_sf) + geom_sf(data = sites_sf) + geom_sf(data = plots_sf, shape = 3) + coord_sf(datum = 4326) + xlim(ext(elev_snow_stk)[c(1,2)]) + ylim(ext(elev_snow_stk)[c(3,4)]) Let’s start on some analysis and computations that we can run on these data. 4.1 Selecting Attributes Perhaps you have vector data and you want to select only certain attributes or attributes that reach a focal threshold. To do so we need to set up a logical statement, and we can do this in base R or in tidyverse. Let’s say we want to select boundaries that are operated by BLM. In the shapefile of management boundaries, this information is located in the column “AGENCY” unique(ownership_sf$AGENCY) ## [1] &quot;BLM&quot; &quot;BR&quot; &quot;DOD&quot; &quot;DOE&quot; &quot;NPS&quot; &quot;Private&quot; &quot;USFS&quot; ## [8] &quot;USFWS&quot; &quot;DNR&quot; &quot;OS&quot; &quot;SITLA&quot; &quot;UDOT&quot; &quot;Tribal&quot; In base R we can use the function which and in tidyverse we can use the function filter # base R blm_boundary &lt;- ownership_sf[ownership_sf$AGENCY == &quot;BLM&quot;, ] # alternatively you can use the base R function subset() blm_boundary &lt;- subset(ownership_sf, ownership_sf$AGENCY == &quot;BLM&quot;) # tidyverse blm_boundary &lt;- ownership_sf %&gt;% filter(AGENCY == &quot;BLM&quot;) ggplot() + geom_sf(data = ownership_sf, col = &quot;grey&quot;, size = 0.1) + geom_sf(data = blm_boundary, fill = &quot;red&quot;, col = &quot;grey30&quot;, alpha = 0.8, size = 0.1) Using these functions, you can set up any logical statement using ==, %in%, &gt;, &gt;=, &lt;, &lt;=, or ! and select for the specific attributes you need. 4.2 Select features by location Let’s make select the management boundaries based on if they are intersected by a major highway. For sf we’ll use the function st_intersect ownership_roads &lt;- st_intersects(interstate_sf_proj, ownership_sf) # the first argument is the target shape and the second argument the shape we&#39;re selecting from class(ownership_roads) ## [1] &quot;sgbp&quot; &quot;list&quot; The output is an sgbp object, or “Sparse Geometry Binary Predicate”. Basically it returns a list of vectors of integers, which refer to the indices of each polygon that intersects. dim(ownership_roads) ## [1] 1848 15197 nrow(interstate_sf_proj) ## [1] 1848 nrow(ownership_sf) ## [1] 15197 So the dimensions of this list are the the number of rows in the target shape (the highways) and the number of rows in the intersecting shape (the management boundaries). Lets look at the first five elements of this list: ownership_roads[1:5] ## [[1]] ## [1] 4687 ## ## [[2]] ## [1] 6408 ## ## [[3]] ## [1] 12997 ## ## [[4]] ## [1] 6406 ## ## [[5]] ## [1] 4687 This means that the 1st row of the road polyline intersects with the 6459th row of the management polygon, the 2nd row of the road polyline intersects with the 6455th row of the management polygon, and etc. If we wanted to know the specifc index of a specific road that intersected with a management boundary, it would be useful to keep all of these indices seperate. Since we just want to know which boundaries intersect a road, we can collapse this whole list together. ownership_roads_index &lt;- unique(unlist(ownership_roads)) # just pull the unique indices ownership_roads_intersect &lt;- ownership_sf[ownership_roads_index, ] ggplot() + geom_sf(data = ownership_sf, col = &quot;grey&quot;, size = 0.1) + geom_sf(data = ownership_roads_intersect, fill = &quot;red&quot;, col = &quot;grey30&quot;, alpha = 0.8, size = 0.1) + geom_sf(data = interstate_sf_proj, col = &quot;black&quot;, size = 1) If you look at the help file for ?st_intersects, you’ll see there are a lot of different functions that select features based on another feature. 4.3 Joining Attributes Let’s load in a table of some data collected at each plot ## Plots Species Date AboveGroundBiomass MeanHeight PercentCover ## 1 A R. maritimus 2021-05-01 20.597457 1.930570 82.09463 ## 2 A B. cernua 2021-05-01 49.769924 2.410401 78.93562 ## 3 A S. acutus 2021-05-01 93.470523 3.342334 47.76196 ## 4 B R. maritimus 2021-05-02 9.946616 1.695365 20.26923 ## 5 B B. cernua 2021-05-02 91.287592 4.460992 23.96294 ## 6 B S. acutus 2021-05-02 25.801678 2.173297 79.73088 Let’s join this table to the Plots feature so we could do some spatial analysis and mapping of the collected data. To join two tables together, we need to input the two tables and the name of the column that exists in both tables (so the join function knows how to match attributes together). In this case, that would be the Plots column. head(plots_sf$Plots) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; head(plot_data$Plots) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; We can use the tidyverse’s join functions. (If you don’t know how joins work, I would recommend looking at the help file by typing ?left_join in the console) plots_join &lt;- left_join(plots_sf, plot_data, by = &quot;Plots&quot;) head(plots_join) ## Simple feature collection with 6 features and 6 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -111.5881 ymin: 40.53102 xmax: -111.5881 ymax: 40.53102 ## Geodetic CRS: WGS 84 ## Plots Species Date AboveGroundBiomass MeanHeight PercentCover ## 1 A R. maritimus 2021-05-01 20.59746 1.93057046 82.09463 ## 2 A B. cernua 2021-05-01 49.76992 2.41040058 78.93562 ## 3 A S. acutus 2021-05-01 93.47052 3.34233369 47.76196 ## 4 A R. maritimus 2021-05-08 17.65568 0.06695167 64.70602 ## 5 A B. cernua 2021-05-08 71.76185 2.99782913 2.33312 ## 6 A S. acutus 2021-05-08 21.21425 3.97119930 86.12095 ## geometry ## 1 POINT (-111.5881 40.53102) ## 2 POINT (-111.5881 40.53102) ## 3 POINT (-111.5881 40.53102) ## 4 POINT (-111.5881 40.53102) ## 5 POINT (-111.5881 40.53102) ## 6 POINT (-111.5881 40.53102) Great! At this point you could then do some spatial analysis based on location, or make a map based on average biomass, for example. However, that’s outside the scope of this workshop. Joining two tables together is a valuable tool to know, not just for GIS but for any data management. 4.4 Extract Raster Values What if we need to get data from our rasters at our specific site locations? We can use the function extract(). Let’s load a landcover raster so we can classify the habitat types of our sites landcover &lt;- rast(&quot;Data/Examples/landcover.tif&quot;) landcover ## class : SpatRaster ## dimensions : 18675, 14838, 1 (nrow, ncol, nlyr) ## resolution : 30, 30 (x, y) ## extent : 229319.6, 674459.6, 4094414, 4654664 (xmin, xmax, ymin, ymax) ## coord. ref. : NAD83 / UTM zone 12N (EPSG:26912) ## source : landcover.tif ## name : landcover ## min value : 137 ## max value : 584 plot(landcover) plot(st_geometry(sites_sf_proj), pch = 16, add = T) extract returns a vector whose indices match the indices of the spatial object. We could leave it as a vector, or we could automatically attach it to the dataframe using $ sites_sf_proj$land_value &lt;- terra::extract(landcover, sites_sf_proj)$landcover (Note that I put terra:: in front of extract(), that’s because there are multiple packages that have a function called extract(), so we want to specify to R which pacakage we want) sites_sf_proj ## Simple feature collection with 15 features and 2 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: 257254.9 ymin: 4106995 xmax: 634770.7 ymax: 4549014 ## Projected CRS: NAD83 / UTM zone 12N ## First 10 features: ## Site geometry land_value ## 1 1 POINT (506648 4311112) 489 ## 2 2 POINT (364889.8 4474182) 148 ## 3 3 POINT (573146.4 4492646) 155 ## 4 4 POINT (303202.3 4546515) 458 ## 5 5 POINT (512358.1 4514704) 549 ## 6 6 POINT (409973.3 4547926) 557 ## 7 7 POINT (388759.4 4549014) 579 ## 8 8 POINT (634770.7 4350035) 316 ## 9 9 POINT (621249.6 4185280) 158 ## 10 10 POINT (524421.3 4431743) 156 Ok, but what do these numbers mean? Our landcover raster is a categorical raster, so these numbers aren’t actually real numbers but represent a habitat type. Fortunately we have a dataframe indicating what these numbers mean. land_info &lt;- read.csv(&quot;Data/Examples/landcover_info.csv&quot;) head(land_info)[,1:5] ## Value ClassCode ClassName SubClassCode SubClassName ## 1 1 1 Forest &amp; Woodland 1.A Tropical Forest &amp; Woodland ## 2 2 1 Forest &amp; Woodland 1.A Tropical Forest &amp; Woodland ## 3 3 1 Forest &amp; Woodland 1.A Tropical Forest &amp; Woodland ## 4 4 1 Forest &amp; Woodland 1.A Tropical Forest &amp; Woodland ## 5 5 1 Forest &amp; Woodland 1.A Tropical Forest &amp; Woodland ## 6 6 1 Forest &amp; Woodland 1.A Tropical Forest &amp; Woodland The column “Value” corresponds to the cell value we extracted from the raster. We can use what we learned earlier how to join two tables together. In our previous example, the columns we used to join the tables together were named the same. In this case, they’re not: one is called “land_value” and one is called “Value”. We could rename them so that they match. But left_join() has a way of handling and matching columns, even if they’re not named the same. If we check ?left_join, in the “Arguments” section for by, it says “to join by different variables on x and y, use a named vector. For example, by = c(\"a\" = \"b\") will match x$a to y$b.” So we just need to set by = c(\"land_value\" = \"Value\") sites_sf_land &lt;- sites_sf_proj %&gt;% left_join(land_info, c(&quot;land_value&quot; = &quot;Value&quot;)) head(sites_sf_land)[,1:6] ## Simple feature collection with 6 features and 6 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: 303202.3 ymin: 4311112 xmax: 573146.4 ymax: 4547926 ## Projected CRS: NAD83 / UTM zone 12N ## Site land_value ClassCode ClassName ## 1 1 489 3 Desert &amp; Semi-Desert ## 2 2 148 1 Forest &amp; Woodland ## 3 3 155 1 Forest &amp; Woodland ## 4 4 458 2 Shrub &amp; Herb Vegetation ## 5 5 549 4 Polar &amp; High Montane Scrub, Grassland &amp; Barrens ## 6 6 557 7 Agricultural &amp; Developed Vegetation ## SubClassCode SubClassName geometry ## 1 3.B Cool Semi-Desert Scrub &amp; Grassland POINT (506648 4311112) ## 2 1.B Temperate &amp; Boreal Forest &amp; Woodland POINT (364889.8 4474182) ## 3 1.B Temperate &amp; Boreal Forest &amp; Woodland POINT (573146.4 4492646) ## 4 2.C Shrub &amp; Herb Wetland POINT (303202.3 4546515) ## 5 4.B Temperate Alpine to Polar Tundra POINT (512358.1 4514704) ## 6 7.B Herbaceous Agricultural Vegetation POINT (409973.3 4547926) Awesome! Now we know what habitat each of our sites reside in. 4.5 Distance Let’s say we needed to know how far from a major road each of our sites are. We’ll use the function st_distance for our sf objects. We simply need to input the focal feature (the sites) and the feature dist &lt;- st_distance(sites_sf_proj, interstate_sf_proj) dim(dist) ## [1] 15 1848 What did this do? Why are there so many columns? Remember that our Utah highways feature is a polyline, meaning it’s a line of lines. If we look at the dimensions of the highways feature: nrow(interstate_sf_proj) ## [1] 1848 There are 1849 lines (i.e. roads) that make up this whole feature. So st_distance found the distance for each site (the number of rows) for every road (the number of columns). This could be useful information, but presumably we want just the distance of the closest road. Let’s first use st_nearest_feature to find the closest road to each of our sites. near_road &lt;- st_nearest_feature(sites_sf_proj, interstate_sf_proj) near_road ## [1] 1101 1177 1384 799 1473 630 232 702 557 521 529 99 1430 15 557 interstate_sf_proj[near_road,] ## Simple feature collection with 15 features and 9 fields ## Geometry type: LINESTRING ## Dimension: XY ## Bounding box: xmin: 282642.9 ymin: 4137639 xmax: 661451 ymax: 4555607 ## Projected CRS: NAD83 / UTM zone 12N ## First 10 features: ## OBJECTID FULLNAME NAME POSTTYPE SPEED_LMT DOT_HWYNAM DOT_SRFTYP ## 1101 234986 I-70 WB FWY I-70 WB FWY 75 I-70 P ## 1177 248283 I-80 EB FWY I-80 EB FWY 75 I-80 P ## 1384 300050 I-80 EB FWY I-80 EB FWY 75 I-80 P ## 799 163766 I-80 WB FWY I-80 WB FWY 75 I-80 P ## 1473 320716 I-80 EB FWY I-80 EB FWY 65 I-80 P ## 630 130269 I-15 SB FWY I-15 SB FWY 70 I-15 P ## 232 45243 I-15 SB FWY I-15 SB FWY 70 I-15 P ## 702 143420 I-70 WB FWY I-70 WB FWY 75 I-70 P ## 557 117401 I-70 EB FWY I-70 EB FWY 75 I-70 P ## 521 108444 I-15 NB FWY I-15 NB FWY 65 I-15 P ## DOT_AADT UNIQUE_ID geometry ## 1101 NA 12SWJ07420079_I-70_WB_FWY LINESTRING (506862.7 430072... ## 1177 12000 12TUL77660482_I-80 EB_FWY LINESTRING (377489.8 450489... ## 1384 16000 12TVL86655323_I-80 EB_FWY LINESTRING (484274.7 455222... ## 799 NA 12TTL95291161_I-80 WB_FWY LINESTRING (282642.9 451236... ## 1473 15000 12TVL67382316_I-80 EB_FWY LINESTRING (465971.3 451778... ## 630 NA 12TVL499657376_I-15 SB_FWY LINESTRING (415374.9 455118... ## 232 NA 12TVL13925335_I-15 SB_FWY LINESTRING (414453.3 455253... ## 702 NA 12SXJ59523424_I-70 WB_FWY LINESTRING (657315.1 433125... ## 557 10000 12SXJ29161112_I-70 EB_FWY LINESTRING (620558.6 431124... ## 521 104000 12TVK44904321_I-15_NB_FWY LINESTRING (444841.6 444243... Now we have the indices of the roads that are closest to our sites, so we can find the distance of just these roads. dist_near_road &lt;- st_distance(sites_sf_proj, interstate_sf_proj[near_road,]) dist_near_road[, 1:5] ## Units: [m] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 10315.51 232582.47 242147.42 281846.56 210641.52 ## [2,] 224150.99 33194.84 142629.52 67810.07 110086.22 ## [3,] 202785.29 195679.41 105586.36 265837.33 109558.50 ## [4,] 319202.58 85153.11 181162.34 35259.29 164053.11 ## [5,] 213951.35 134881.34 46277.52 204448.52 45475.54 ## [6,] 265510.76 53802.73 74425.61 108507.81 59486.92 ## [7,] 274946.67 45537.59 95569.24 89311.68 80131.79 ## [8,] 136172.75 299925.04 251423.12 364314.12 237979.89 ## [9,] 161631.44 401648.70 391502.51 451943.09 366979.12 ## [10,] 132044.13 163762.97 126873.05 230529.64 104020.44 ## [11,] 34586.77 269676.04 268322.22 321111.19 239842.61 ## [12,] 250038.01 249684.87 349826.95 227716.46 311863.50 ## [13,] 244382.54 398276.49 462818.09 407078.61 424773.11 ## [14,] 193621.33 184870.40 278450.62 180463.52 240048.54 ## [15,] 138475.56 365526.65 342070.17 421508.03 321034.17 dim(dist_near_road) ## [1] 15 15 This is still giving us the distance of each road to each site but we just want the distance between each site and it’s nearest road. There’s an argument in st_distance() called by_element that tells st_distance() to only find the distance between the first elements of the two objects. dist_near_road &lt;- st_distance(sites_sf_proj, interstate_sf_proj[near_road,], by_element = TRUE) dist_near_road ## Units: [m] ## [1] 10315.506 33194.842 105586.359 35259.292 45475.541 6297.237 ## [7] 25526.749 29201.908 125931.670 80267.682 16154.905 102959.472 ## [13] 65466.728 62646.964 65802.131 There we go! Now we have the distance between each site and it’s nearest road. This output is a vector, so we can add it to our sites data frame with $ (or mutate() in tidyverse) sites_sf_proj$dist_near_road &lt;- dist_near_road head(sites_sf_proj) ## Simple feature collection with 6 features and 3 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: 303202.3 ymin: 4311112 xmax: 573146.4 ymax: 4547926 ## Projected CRS: NAD83 / UTM zone 12N ## Site geometry land_value dist_near_road ## 1 1 POINT (506648 4311112) 489 10315.506 [m] ## 2 2 POINT (364889.8 4474182) 148 33194.842 [m] ## 3 3 POINT (573146.4 4492646) 155 105586.359 [m] ## 4 4 POINT (303202.3 4546515) 458 35259.292 [m] ## 5 5 POINT (512358.1 4514704) 549 45475.541 [m] ## 6 6 POINT (409973.3 4547926) 557 6297.237 [m] sites_sf_proj &lt;- sites_sf_proj %&gt;% mutate(dist_near_road = dist_near_road) (Note that if you look at the help file i.e. ?st_distance, there are other functions to calculate geometric measurements for sf objects: st_area and st_length) 4.6 Calculate Terrain Characteristics From a DEM (digital elevation model) we can obtain a lot of other rasters that are likely useful in GIS research. The elevation raster we’ve been working with is a DEM. From a DEM we can derive other terrain characteristics : Slope: Measurement of “steepness” Aspect: Measurements of “Northness” and “Eastness” Flow direction of water: the direction of the greatest drop in elevation Terrain Ruggedness Index (TRI): the mean of the absolute differences between the value of a cell and the value of its 8 surrounding cells Topographic Position Index (TPI): the difference between the value of a cell and the mean value of its 8 surrounding cells Roughness: the difference between the maximum and the minimum value of a cell and its 8 surrounding cells These definitions came from the help file for the function we can use to derive these characteristics: terrain(). slope &lt;- terrain(elev_snow_stk$elevation, v = &quot;slope&quot;, unit = &quot;radians&quot;) aspect &lt;- terrain(elev_snow_stk$elevation, v = &quot;aspect&quot;, unit = &quot;radians&quot;) roughness &lt;- terrain(elev_snow_stk$elevation, v = &quot;roughness&quot;) terrain_stk &lt;- c(elev_snow_stk$elevation, slope, aspect, roughness) terrain_stk ## class : SpatRaster ## dimensions : 240, 240, 4 (nrow, ncol, nlyr) ## resolution : 0.008333333, 0.008333333 (x, y) ## extent : -112, -110, 40, 42 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## sources : elev_snow_nw_stack.tif ## memory ## memory ## memory ## names : elevation, slope, aspect, roughness ## min values : 1280.021, 0.0000000, 3.786645e-05, 0.000 ## max values : 4087.231, 0.5922148, 6.283185e+00, 1305.816 To compute the Northness or Eastness of a cell, we actually have to do one more step to the aspect raster. Aspect is a circular measurement (which is why its units are in degrees or radians), so (if you remember how trigonometry works) to calculate northness and eastness we need to use cosine and sine respectively. Because our units are in radians, we can simply apply the cos() and sin() functions directly to the aspect raster. aspect_cos &lt;- cos(terrain_stk$aspect) aspect_sin &lt;- sin(terrain_stk$aspect) aspect_stk &lt;- c(aspect_cos, aspect_sin) names(aspect_stk) &lt;- c(&quot;cosine_northness&quot;, &quot;sine_eastness&quot;) aspect_stk ## class : SpatRaster ## dimensions : 240, 240, 2 (nrow, ncol, nlyr) ## resolution : 0.008333333, 0.008333333 (x, y) ## extent : -112, -110, 40, 42 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## source(s) : memory ## names : cosine_northness, sine_eastness ## min values : -1, -1 ## max values : 1, 1 plot(aspect_stk) 4.7 Re-Classify Rasters Sometimes you may need to re-classify a raster. This means that you are assigning or changing raster cell values to new values. You may need to do this to replace values with new information group or bin specific values together set values to NA or set NA cells to a value To re-classify a raster in R you use the aptly named function reclassify(). The arguments for reclassify are x (the Raster* object) and rcl (a matrix for reclassification) Let’s practice. Let’s say we want to simplify our landcover raster and classify the cells into more general habitat classifications. Fortunately, the landcover data frame already has this information unique(land_info[, c(&quot;ClassCode&quot;, &quot;ClassName&quot;)]) ## ClassCode ClassName ## 1 1 Forest &amp; Woodland ## 116 2 Shrub &amp; Herb Vegetation ## 299 3 Desert &amp; Semi-Desert ## 489 4 Polar &amp; High Montane Scrub, Grassland &amp; Barrens ## 498 5 Aquatic Vegetation ## 503 6 Open Rock Vegetation ## 541 8 Nonvascular &amp; Sparse Vascular Rock Vegetation ## 544 7 Agricultural &amp; Developed Vegetation ## 547 9 Introduced &amp; Semi Natural Vegetation ## 553 10 Recently Disturbed or Modified ## 565 11 Open Water ## 568 8 Developed &amp; Other Human Use summary(land_info[, c(&quot;Value&quot;, &quot;ClassCode&quot;)]) ## Value ClassCode ## Min. : 1.0 Min. : 1.000 ## 1st Qu.:144.8 1st Qu.: 1.000 ## Median :293.5 Median : 2.000 ## Mean :292.3 Mean : 2.257 ## 3rd Qu.:440.2 3rd Qu.: 2.000 ## Max. :584.0 Max. :11.000 The “Value” column (the data representing the cell values of the landcover raster) has values from 1 to 584 to represent specific habitats within “Forest &amp; Woodland” or “Desert &amp; Semi-Desert” for example, while the “ClassCode” column only has values from 1 to 11 to represent these general habitat types. Much more simple. So let’s take these two columns and make a matrix for the rcl argument in reclassify. rclLand &lt;- land_info[, c(&quot;Value&quot;, &quot;ClassCode&quot;)] rclLand &lt;- as.matrix(rclLand) head(rclLand) ## Value ClassCode ## [1,] 1 1 ## [2,] 2 1 ## [3,] 3 1 ## [4,] 4 1 ## [5,] 5 1 ## [6,] 6 1 Now we just need to input this matrix and the landcover raster into the function. landcover_rcl &lt;- classify(landcover, rclLand) names(landcover_rcl) &lt;- &quot;landcover_rcl&quot; landcover_rcl ## class : SpatRaster ## dimensions : 18675, 14838, 1 (nrow, ncol, nlyr) ## resolution : 30, 30 (x, y) ## extent : 229319.6, 674459.6, 4094414, 4654664 (xmin, xmax, ymin, ymax) ## coord. ref. : NAD83 / UTM zone 12N (EPSG:26912) ## source : landcover_rcl.tif ## name : landcover_rcl ## min value : 1 ## max value : 11 plot(c(landcover, landcover_rcl)) # plot them together to compare Great! 4.8 Raster Cell Stats In my research I often have to perform cell algebra or focal statistics. Maybe you need to know the average elevation or the total herbaceous biomass within a certain area. The way to get these values are with the function cellStats. We simply need to input the raster and the stat function: sum, mean, min, max, sd, or a homemade function. Let’s say we need to calculate some stats of the elevation, SWE, and snow depth within a 5-km area around our sites. plot(elev_snow_stk$elevation) plot(st_geometry(sites_sf), pch = 16, add = T) We need to know which of our sites are within the extent of the raster. There are probably many ways of doing this. The most straightforward way I can think of getting just the sites that fall within the raster are 1) extracting raster data at each site and 2) filtering to just the sites that have raster data attached. sites_rast &lt;- terra::extract(elev_snow_stk, sites_sf) sites_rast # notice how only 3 rows (sites) have data ## ID elevation swe snow_depth ## 1 1 NA NA NA ## 2 2 NA NA NA ## 3 3 2585.951 127 542 ## 4 4 NA NA NA ## 5 5 3218.554 502 1640 ## 6 6 NA NA NA ## 7 7 NA NA NA ## 8 8 NA NA NA ## 9 9 NA NA NA ## 10 10 2452.774 109 521 ## 11 11 NA NA NA ## 12 12 NA NA NA ## 13 13 NA NA NA ## 14 14 NA NA NA ## 15 15 NA NA NA sites_sf &lt;- left_join(sites_sf, as.data.frame(sites_rast), by = join_by(&quot;Site&quot; == &quot;ID&quot;)) sites_sf ## Simple feature collection with 15 features and 4 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -113.7909 ymin: 37.09849 xmax: -109.4373 ymax: 41.08479 ## Geodetic CRS: WGS 84 ## First 10 features: ## Site elevation swe snow_depth geometry ## 1 1 NA NA NA POINT (-110.9233 38.94893) ## 2 2 NA NA NA POINT (-112.5923 40.4073) ## 3 3 2585.951 127 542 POINT (-110.1357 40.58137) ## 4 4 NA NA NA POINT (-113.3415 41.04609) ## 5 5 3218.554 502 1640 POINT (-110.8535 40.78323) ## 6 6 NA NA NA POINT (-112.0717 41.07762) ## 7 7 NA NA NA POINT (-112.3244 41.08479) ## 8 8 NA NA NA POINT (-109.4373 39.28921) ## 9 9 NA NA NA POINT (-109.6226 37.80686) ## 10 10 2452.774 109 521 POINT (-110.7138 40.03556) sites_sf_rast &lt;- sites_sf %&gt;% filter(!is.na(elevation) &amp; !is.na(swe) &amp; !is.na(snow_depth)) sites_sf_rast ## Simple feature collection with 3 features and 4 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -110.8535 ymin: 40.03556 xmax: -110.1357 ymax: 40.78323 ## Geodetic CRS: WGS 84 ## Site elevation swe snow_depth geometry ## 1 3 2585.951 127 542 POINT (-110.1357 40.58137) ## 2 5 3218.554 502 1640 POINT (-110.8535 40.78323) ## 3 10 2452.774 109 521 POINT (-110.7138 40.03556) Let’s just do this for one of our sites for now and then we can try it with the others. site_sf_rast_01 &lt;- sites_sf_rast[1,] First, we need to know which parts of this raster are within 5-km of the site, so we will need to crop the raster. We can do that by first making a buffer with st_buffer() site_buff_5km &lt;- st_buffer(site_sf_rast_01, 5000) # units are in meters, 1000m = 1km plot(elev_snow_stk[[1]]) # let&#39;s just plot one layer for demonstration plot(st_geometry(site_buff_5km), lwd = 1, add = T) Now let’s crop the raster to this buffer’s extent. We’ll use the function crop(), which takes in the raster object we want to crop and an “extent object, or any object from which an Extent object can be extracted” (quoted from the crop help file). Because we can make an extent object from our buffer ext(site_buff_5km) ## SpatExtent : -110.195356322096, -110.076232859429, 40.5357574438391, 40.626942424945 (xmin, xmax, ymin, ymax) We can input our buffer object directly into crop() stack_crop &lt;- crop(elev_snow_stk, site_buff_5km) plot(stack_crop[[1]]) plot(st_geometry(site_buff_5km), add = T) plot(st_geometry(site_sf_rast_01), pch = 16, add = T) Great! Now let’s calculate the mean, median, min/max, and standard deviation of the elevation, SWE, and snow depth within this area. stack_stats &lt;- data.frame( mean_5km = global(stack_crop, fun = &quot;mean&quot;), min_5km = global(stack_crop, fun = &quot;min&quot;), max_5km = global(stack_crop, fun = &quot;max&quot;), sd_5km = global(stack_crop, fun = &quot;sd&quot;)) # let&#39;s add a column for the environment type from the raster and remove the row names stack_stats$environment &lt;- row.names(stack_stats) row.names(stack_stats) &lt;- NULL stack_stats ## mean min max sd environment ## 1 2522.8263 2138.969 3138.245 234.83332 elevation ## 2 140.4935 55.000 282.000 51.01483 swe ## 3 614.5195 284.000 1079.000 175.49777 snow_depth We have our stats, but we want to join them with our sites data frame to make them more meaningful. We can’t use the function cbind() because the number of rows don’t match up. Instead, we can use dplyr’s function bind_cols(), which works just like cbind() or do.call(cbind, x) except that it’s a bit smarter and can add NAs if the number of rows or columns doesn’t match. bind_cols(site_sf_rast_01, stack_stats) ## Simple feature collection with 3 features and 9 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -110.1357 ymin: 40.58137 xmax: -110.1357 ymax: 40.58137 ## Geodetic CRS: WGS 84 ## Site elevation swe snow_depth mean min max sd ## 1 3 2585.951 127 542 2522.8263 2138.969 3138.245 234.83332 ## 2 3 2585.951 127 542 140.4935 55.000 282.000 51.01483 ## 3 3 2585.951 127 542 614.5195 284.000 1079.000 175.49777 ## environment geometry ## 1 elevation POINT (-110.1357 40.58137) ## 2 swe POINT (-110.1357 40.58137) ## 3 snow_depth POINT (-110.1357 40.58137) Great! But we only did this process for one site when presumabaly we would want these data for all available sites. We could copy and paste for each site, but that can get tedious, not to mention out of control if we had more than 3 sites. So the best way to do this would be with loops. 4.9 A Note About Loops Learning all these functions is all well and good, but what if you have to perform them all on multiple features or rasters? Copying and pasting code over and over again can soon become confusing and messy and cause your code to be inefficient. The better way to address this is (in my opinion) with loops! for loops and lapply are lifesavers and I use them in all of my code. A future workshop will go into more depth on how to use loops (be on the lookout for that!), so I won’t go over them in too much detail. But I do want to show some ways you can use them for GIS applications. (These code chunks are for demonstration only, these data and directories don’t actually exist) # Example 1: Load a set of shapefiles and find the area for each filenames &lt;- list.files(dir) # get a list of shapefile names in a directory area_list &lt;- c() # create an empty vector for the areas to live in for(i in 1:length(filenames)){ # load the shapefile shp &lt;- st_read(filenames[i]) # calculate the area area &lt;- st_area(shp) # put the area into the vector area_list &lt;- c(area_list, area) } # -----------------------------------------------------------------------------X # Example 2: Load a set of shapefiles, generate a buffer for each, and calculate the # mean value of a raster within that buffer and the focal feature filenames &lt;- list.files(dir) # get a list of shapefile names in a directory r &lt;- raster(raster_filename) # load a raster lapply(filenames, function(fn){ # load a shapefile shp &lt;- st_read(fn) # generate a 10kmX10km buffer buffer &lt;- st_buffer(shp, dist = 10000) # crop the raster to the shape and the buffer r_shp &lt;- crop(r, extent(shp)) r_buffer &lt;- crop(r, extent(buffer)) # calculate the mean value of the raster within the buffer and the feature r_shp_mean &lt;- cellStats(r_shp, stat = &quot;mean&quot;, na.rm = TRUE) r_buff_mean &lt;- cellStats(r_shp, stat = &quot;mean&quot;, na.rm = TRUE) # return both means in a list return(list(r_shp_mean, r_buff_mean)) }) # -----------------------------------------------------------------------------X # Example 3: Generate a raster of the sum from a set of RasterStacks # and then save the output raster filenames &lt;- list.files(dir) # get a list of raster files in a directory out_names &lt;- paste0(filenames, &quot;_sum&quot;) lapply(1:length(filenames), function(i){ # load RasterStak stk &lt;- stack(filenames[i]) # create a raster that is the sum of all layers in the stack sum &lt;- calc(stk, fun = sum) sum &lt;- sum(stk) # these two operations are equivalent writeRaster(sum, out_names[i], format = &quot;GTiff&quot;) }) # -----------------------------------------------------------------------------X # Example 4: Pull the number of zeros in a set of rasters filenames &lt;- list.files(dir) # get a list of raster files in a directory lapply(filenames, function(fn){ # load raster rast &lt;- raster(fn) # get the number of zeros in the raster n_0 &lt;- getValues(rast) == 0 %&gt;% which() %&gt;% length() return(n_0) }) Even more efficient would be to run these in parallel, but that is way beyond the scope of this workshop I hope these functions helped you! The next chapter goes over some ways of obtaining the data we worked on today. "],["making-pretty-maps.html", "Chapter 5 Making Pretty Maps 5.1 USA Population 5.2 Yellowstone 5.3 Landcover", " Chapter 5 Making Pretty Maps Now that we’ve learned how to do some spatial analysis, you likely want to make a nice looking map to put in a manuscript or presentation. Note: This workshop is currently a work in progress. All of the mapping elements are present, but the presentation and setup portion are yet to be added. I appreciate your patience as I finish this up in my free time. library(sf) library(tidyverse) library(terra) 5.1 USA Population # Load the US population shapefile usa &lt;- st_read(&quot;Data/Visualization/US_states&quot;, quiet = T) head(usa) ## Simple feature collection with 6 features and 5 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -124.3834 ymin: 30.24071 xmax: -71.78015 ymax: 42.04937 ## Geodetic CRS: WGS 84 ## STATE POP2020 POP2010 DIVISION REGION ## 1 Alabama 5024279 4779736 East South Central South ## 2 Arizona 7151502 6392017 Mountain West ## 3 Arkansas 3011524 2915918 West South Central South ## 4 California 39538223 37253956 Pacific West ## 5 Colorado 5773714 5029196 Mountain West ## 6 Connecticut 3605944 3574097 New England Northeast ## geometry ## 1 MULTIPOLYGON (((-87.46201 3... ## 2 MULTIPOLYGON (((-114.6374 3... ## 3 MULTIPOLYGON (((-94.05103 3... ## 4 MULTIPOLYGON (((-120.006 42... ## 5 MULTIPOLYGON (((-102.0552 4... ## 6 MULTIPOLYGON (((-73.49902 4... # plot to see what it looks like ggplot(usa) + geom_sf() # Fill the states based on their population ggplot(usa) + geom_sf(aes(fill = POP2020)) # Add Canada and Mexico to the map # Adds context for the US (isn&#39;t floating in some void but exists in the world) library(maps) can_mex &lt;- maps::map(&quot;world&quot;, plot = F, fill = T) %&gt;% st_as_sf() %&gt;% st_transform(4326) %&gt;% filter(ID %in% c(&quot;Canada&quot;, &quot;Mexico&quot;)) ggplot(can_mex) + geom_sf() ggplot(usa) + # make the outline around the states white geom_sf(aes(fill = POP2020), col = &quot;white&quot;) + # remove the outline around Canada and Mexico and grey them out # (focuses the eye towards what&#39;s important about this map ie US population) geom_sf(data = can_mex, col = NA, fill = &quot;grey80&quot;) # Add North American lakes to the map lakes &lt;- maps::map(&quot;lakes&quot;, plot = F, fill = T) %&gt;% st_as_sf() %&gt;% st_transform(4326) st_bbox(bind_rows(usa, can_mex)) ## xmin ymin xmax ymax ## -179.23109 18.86546 -67.00742 71.43979 na_lakes &lt;- lakes %&gt;% st_crop(st_bbox(bind_rows(usa, can_mex))) ggplot(na_lakes) + geom_sf() ggplot(usa) + geom_sf(aes(fill = POP2020), col = &quot;white&quot;) + geom_sf(data = can_mex, col = NA, fill = &quot;grey80&quot;) + geom_sf(data = na_lakes, fill = &quot;white&quot;, col = NA) + theme_bw() # Filter out Alaska and Hawaii to create just the contiguous USA polygons contig_usa &lt;- usa %&gt;% filter(!STATE %in% c(&quot;Alaska&quot;, &quot;Hawaii&quot;)) ggplot(contig_usa) + geom_sf() # Project the contiguous USA to Conus Albers (EPSG:6350) # Standard projection for US maps contig_usa &lt;- st_transform(contig_usa, 6350) ggplot(contig_usa) + geom_sf() # Project Canada, Mexico, and the lakes to Conus Albers and add to the map can_mex &lt;- st_transform(can_mex, 6350) lakes &lt;- st_transform(lakes, 6350) ggplot(contig_usa) + geom_sf(aes(fill = POP2020), col = &quot;white&quot;) + geom_sf(data = can_mex, col = NA, fill = &quot;grey80&quot;) + geom_sf(data = na_lakes, fill = &quot;white&quot;, col = NA) + theme_bw() # Zoom the view window to just the contiguous USA st_bbox(contig_usa) ## xmin ymin xmax ymax ## -2353797.2 321803.8 2254131.0 3163215.2 us_plot &lt;- ggplot(contig_usa) + geom_sf(aes(fill = POP2020), col = &quot;white&quot;) + geom_sf(data = can_mex, col = NA, fill = &quot;grey80&quot;) + geom_sf(data = na_lakes, fill = &quot;white&quot;, col = NA) + theme_bw() + xlim(c(st_bbox(contig_usa)[&quot;xmin&quot;], st_bbox(contig_usa)[&quot;xmax&quot;])) + ylim(c(st_bbox(contig_usa)[&quot;ymin&quot;] - 1e5, st_bbox(contig_usa)[&quot;ymax&quot;] + 1e6)) us_plot # Filter to create an Alaska and a Hawaii polygon # We&#39;ll add these to the map as inset maps alaska &lt;- filter(usa, STATE == &quot;Alaska&quot;) alaska &lt;- st_transform(alaska, 3338) ak_plot &lt;- ggplot(alaska) + geom_sf(aes(fill = POP2020), col = &quot;white&quot;) + geom_sf(data = can_mex, col = NA, fill = &quot;grey80&quot;) + geom_sf(data = na_lakes, fill = &quot;white&quot;, col = NA) + theme_bw() + xlim(c(st_bbox(alaska)[&quot;xmin&quot;], st_bbox(alaska)[&quot;xmax&quot;])) + ylim(c(st_bbox(alaska)[&quot;ymin&quot;], st_bbox(alaska)[&quot;ymax&quot;])) + coord_sf(datum = 3338) ak_plot hawaii &lt;- usa %&gt;% filter(STATE == &quot;Hawaii&quot;) %&gt;% st_transform(st_crs(&quot;ESRI:102007&quot;)) hi_plot &lt;- ggplot(hawaii) + geom_sf(aes(fill = POP2020), col = &quot;white&quot;) + theme_bw() + xlim(c(st_bbox(hawaii)[&quot;xmin&quot;], st_bbox(hawaii)[&quot;xmax&quot;])) + ylim(c(st_bbox(hawaii)[&quot;ymin&quot;], st_bbox(hawaii)[&quot;ymax&quot;])) hi_plot # Use the library &#39;cowplot&#39; to add figures on top of other figures library(cowplot) # &#39;ggdraw&#39; is cowplot&#39;s version of &#39;ggplot&#39; # &#39;draw_plot&#39; is cowplot&#39;s version of &#39;geom_sf&#39; ggdraw() + draw_plot(us_plot) + # the x and y arguments tell cowplot where these figures should go on # the map (based on, I beleive, the bottom left corner) # The units are in pixels from 0 (bottom/left) to 1 (top/right) # The &#39;height&#39; argument tells cowplot how big/small this figure should be # NOTE: these values are very subjective to your plotting window. If you # are saving this as a picture file, be sure to check frequently that it looks # right, because it will likely look different than in your plotting window # in R studio draw_plot(ak_plot, x = 0.035, y = 0.75, width = 0.25, height = 0.25) + draw_plot(hi_plot, x = 0.035, y = 0.015, width = 0.25, height = 0.25) # Right now this doesn&#39;t look good. We&#39;ll need to remove the legend from the # Alaska and Hawaii maps. But we also want to be sure that AK and HI&#39;s population # is shown visually on the same scale as the contiguous US (right now they&#39;re) # on their own scales, so the color shown is mis-representing the population # Let&#39;s bin the US populations in a way that makes more sense to look at # &#39;BAMMtools&#39; has a function for Jenks Breaks, which is a standard way to bin # continuous values library(BAMMtools) (pop_breaks &lt;- getJenksBreaks(var = usa$POP2020, k = 6)) ## [1] 576851 3605944 7705281 13002700 29145505 39538223 (pop_breaks &lt;- signif(pop_breaks, 2)) ## [1] 5.8e+05 3.6e+06 7.7e+06 1.3e+07 2.9e+07 4.0e+07 # Add a flag for each state to mark which population bin it should go in usa &lt;- usa %&gt;% mutate(POP2020_BREAKS = case_when( POP2020 &gt;= min(POP2020) &amp; POP2020 &lt; pop_breaks[2] ~ &quot;1&quot;, POP2020 &gt;= pop_breaks[2] &amp; POP2020 &lt; pop_breaks[3] ~ &quot;2&quot;, POP2020 &gt;= pop_breaks[3] &amp; POP2020 &lt; pop_breaks[4] ~ &quot;3&quot;, POP2020 &gt;= pop_breaks[4] &amp; POP2020 &lt; pop_breaks[5] ~ &quot;4&quot;, POP2020 &gt;= pop_breaks[5] &amp; POP2020 &lt;= max(POP2020) ~ &quot;5&quot;)) # Now we&#39;ve binned population into categories ggplot(usa) + geom_sf(aes(fill = POP2020_BREAKS)) # The base color palette can be a little ugly # hcl.pals() gives you a list of pre-made palettes, but you could also # make your own hcl.pals() ## [1] &quot;Pastel 1&quot; &quot;Dark 2&quot; &quot;Dark 3&quot; &quot;Set 2&quot; ## [5] &quot;Set 3&quot; &quot;Warm&quot; &quot;Cold&quot; &quot;Harmonic&quot; ## [9] &quot;Dynamic&quot; &quot;Grays&quot; &quot;Light Grays&quot; &quot;Blues 2&quot; ## [13] &quot;Blues 3&quot; &quot;Purples 2&quot; &quot;Purples 3&quot; &quot;Reds 2&quot; ## [17] &quot;Reds 3&quot; &quot;Greens 2&quot; &quot;Greens 3&quot; &quot;Oslo&quot; ## [21] &quot;Purple-Blue&quot; &quot;Red-Purple&quot; &quot;Red-Blue&quot; &quot;Purple-Orange&quot; ## [25] &quot;Purple-Yellow&quot; &quot;Blue-Yellow&quot; &quot;Green-Yellow&quot; &quot;Red-Yellow&quot; ## [29] &quot;Heat&quot; &quot;Heat 2&quot; &quot;Terrain&quot; &quot;Terrain 2&quot; ## [33] &quot;Viridis&quot; &quot;Plasma&quot; &quot;Inferno&quot; &quot;Rocket&quot; ## [37] &quot;Mako&quot; &quot;Dark Mint&quot; &quot;Mint&quot; &quot;BluGrn&quot; ## [41] &quot;Teal&quot; &quot;TealGrn&quot; &quot;Emrld&quot; &quot;BluYl&quot; ## [45] &quot;ag_GrnYl&quot; &quot;Peach&quot; &quot;PinkYl&quot; &quot;Burg&quot; ## [49] &quot;BurgYl&quot; &quot;RedOr&quot; &quot;OrYel&quot; &quot;Purp&quot; ## [53] &quot;PurpOr&quot; &quot;Sunset&quot; &quot;Magenta&quot; &quot;SunsetDark&quot; ## [57] &quot;ag_Sunset&quot; &quot;BrwnYl&quot; &quot;YlOrRd&quot; &quot;YlOrBr&quot; ## [61] &quot;OrRd&quot; &quot;Oranges&quot; &quot;YlGn&quot; &quot;YlGnBu&quot; ## [65] &quot;Reds&quot; &quot;RdPu&quot; &quot;PuRd&quot; &quot;Purples&quot; ## [69] &quot;PuBuGn&quot; &quot;PuBu&quot; &quot;Greens&quot; &quot;BuGn&quot; ## [73] &quot;GnBu&quot; &quot;BuPu&quot; &quot;Blues&quot; &quot;Lajolla&quot; ## [77] &quot;Turku&quot; &quot;Hawaii&quot; &quot;Batlow&quot; &quot;Blue-Red&quot; ## [81] &quot;Blue-Red 2&quot; &quot;Blue-Red 3&quot; &quot;Red-Green&quot; &quot;Purple-Green&quot; ## [85] &quot;Purple-Brown&quot; &quot;Green-Brown&quot; &quot;Blue-Yellow 2&quot; &quot;Blue-Yellow 3&quot; ## [89] &quot;Green-Orange&quot; &quot;Cyan-Magenta&quot; &quot;Tropic&quot; &quot;Broc&quot; ## [93] &quot;Cork&quot; &quot;Vik&quot; &quot;Berlin&quot; &quot;Lisbon&quot; ## [97] &quot;Tofino&quot; &quot;ArmyRose&quot; &quot;Earth&quot; &quot;Fall&quot; ## [101] &quot;Geyser&quot; &quot;TealRose&quot; &quot;Temps&quot; &quot;PuOr&quot; ## [105] &quot;RdBu&quot; &quot;RdGy&quot; &quot;PiYG&quot; &quot;PRGn&quot; ## [109] &quot;BrBG&quot; &quot;RdYlBu&quot; &quot;RdYlGn&quot; &quot;Spectral&quot; ## [113] &quot;Zissou 1&quot; &quot;Cividis&quot; &quot;Roma&quot; # We&#39;ll use the palette &#39;Emrld&#39; (but you could pick any you like) pop_cols &lt;- hcl.colors(5, palette = &quot;Emrld&quot;) ggplot(usa) + geom_sf(aes(fill = POP2020_BREAKS)) + scale_fill_manual(values = pop_cols, labels = c(paste(pop_breaks[1], &quot;-&quot;, pop_breaks[2]), paste(pop_breaks[2], &quot;-&quot;, pop_breaks[3]), paste(pop_breaks[3], &quot;-&quot;, pop_breaks[4]), paste(pop_breaks[4], &quot;-&quot;, pop_breaks[5]), paste(pop_breaks[5], &quot;-&quot;, pop_breaks[6]))) # Let&#39;s make the legend label into an object so we don&#39;t have to copy-paste # this everytime we need to plot pop_breaks_labs &lt;- c(paste(pop_breaks[1], &quot;-&quot;, pop_breaks[2]), paste(pop_breaks[2], &quot;-&quot;, pop_breaks[3]), paste(pop_breaks[3], &quot;-&quot;, pop_breaks[4]), paste(pop_breaks[4], &quot;-&quot;, pop_breaks[5]), paste(pop_breaks[5], &quot;-&quot;, pop_breaks[6])) # Let&#39;s make the map of the contiguous US as before # (we need to re-make teh contig_usa object since it doesn&#39;t have the population # breaks attributes) contig_usa &lt;- usa %&gt;% filter(!STATE %in% c(&quot;Alaska&quot;, &quot;Hawaii&quot;)) %&gt;% st_transform(6350) us_pop_plot &lt;- ggplot() + geom_sf(aes(fill = POP2020_BREAKS), col = &quot;white&quot;, data = contig_usa) + geom_sf(data = can_mex, col = NA, fill = &quot;grey80&quot;) + geom_sf(data = na_lakes, fill = &quot;white&quot;, col = NA) + theme_bw() + xlim(c(st_bbox(contig_usa)[&quot;xmin&quot;], st_bbox(contig_usa)[&quot;xmax&quot;])) + ylim(c(st_bbox(contig_usa)[&quot;ymin&quot;] - 1e5, st_bbox(contig_usa)[&quot;ymax&quot;] + 1e6)) + scale_fill_manual(values = pop_cols, labels = pop_breaks_labs) us_pop_plot # Let&#39;s also remake the AK and HI inset maps # (AK and HI need to be re-made because they don&#39;t have the population breaks # attributes) alaska &lt;- usa %&gt;% filter(STATE == &quot;Alaska&quot;) %&gt;% bind_rows(data.frame(POP2020_BREAKS = as.character(1:5))) %&gt;% st_transform(3338) ak_pop_plot &lt;- ggplot() + geom_sf(aes(fill = POP2020_BREAKS), col = &quot;white&quot;, data = alaska) + geom_sf(data = can_mex, col = NA, fill = &quot;grey80&quot;) + geom_sf(data = na_lakes, fill = &quot;white&quot;, col = NA) + theme_bw() + xlim(c(st_bbox(alaska)[&quot;xmin&quot;], st_bbox(alaska)[&quot;xmax&quot;])) + ylim(c(st_bbox(alaska)[&quot;ymin&quot;], st_bbox(alaska)[&quot;ymax&quot;])) + coord_sf(datum = 3338) + scale_fill_manual(values = pop_cols, labels = pop_breaks_labs) ak_pop_plot hawaii &lt;- usa %&gt;% filter(STATE == &quot;Hawaii&quot;) %&gt;% st_transform(st_crs(&quot;ESRI:102007&quot;)) %&gt;% bind_rows(data.frame(POP2020_BREAKS = as.character(1:5))) hi_pop_plot &lt;- ggplot() + geom_sf(aes(fill = POP2020_BREAKS), col = &quot;white&quot;, data = hawaii) + theme_bw() + xlim(c(st_bbox(hawaii)[&quot;xmin&quot;], st_bbox(hawaii)[&quot;xmax&quot;])) + ylim(c(st_bbox(hawaii)[&quot;ymin&quot;], st_bbox(hawaii)[&quot;ymax&quot;])) + scale_fill_manual(values = pop_cols, labels = pop_breaks_labs) hi_pop_plot # Now add everything together us_pop_map &lt;- ggdraw() + draw_plot(us_pop_plot + labs(fill = &quot;Population&quot;)) + # legend.position = &#39;none&#39; removes the legend draw_plot(ak_pop_plot + theme(legend.position = &#39;none&#39;, axis.text = element_blank(), axis.ticks = element_blank(), plot.background = element_blank()), x = 0.0375, y = 0.707, width = 0.25, height = 0.25) + draw_plot(hi_pop_plot + theme(legend.position = &#39;none&#39;, axis.text = element_blank(), axis.ticks = element_blank(), plot.background = element_blank()), x = 0.0375, y = 0.0351, width = 0.2, height = 0.25) us_pop_map # Reminder that the xy and width/height for cowplot plots are very subjective, # So you&#39;ll have to check and adjust for your viewing window 5.1.1 Change in Population For the last plot we visually showed the current US population with colors We can also visually convey information through points and their size For this plot, let’s combine the two to create a map showwing the current US population and it’s change from the last census count # Check the dataframe head(usa) ## Simple feature collection with 6 features and 6 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -124.3834 ymin: 30.24071 xmax: -71.78015 ymax: 42.04937 ## Geodetic CRS: WGS 84 ## STATE POP2020 POP2010 DIVISION REGION ## 1 Alabama 5024279 4779736 East South Central South ## 2 Arizona 7151502 6392017 Mountain West ## 3 Arkansas 3011524 2915918 West South Central South ## 4 California 39538223 37253956 Pacific West ## 5 Colorado 5773714 5029196 Mountain West ## 6 Connecticut 3605944 3574097 New England Northeast ## geometry POP2020_BREAKS ## 1 MULTIPOLYGON (((-87.46201 3... 2 ## 2 MULTIPOLYGON (((-114.6374 3... 2 ## 3 MULTIPOLYGON (((-94.05103 3... 1 ## 4 MULTIPOLYGON (((-120.006 42... 5 ## 5 MULTIPOLYGON (((-102.0552 4... 2 ## 6 MULTIPOLYGON (((-73.49902 4... 2 # we have POP2020 and POP2010, so we can calculate the percent change in population usa &lt;- usa %&gt;% mutate(pop_perc_change = ((POP2020 - POP2010)/POP2010) * 100) summary(usa$pop_perc_change) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -3.199 2.902 5.654 7.018 10.410 18.370 # Plot just to check what it looks like. Reminder that our final plot will # have the color-coded current population (the previous map) and the percent # change in population represented by different size points ggplot(usa) + geom_sf(aes(fill = pop_perc_change)) # Find the centeroid of each state for the point to fall on state_center &lt;- st_centroid(usa) ggplot(usa) + geom_sf(data = usa) + geom_sf(data = state_center) # Show the percent change in population based on the point size ggplot(usa) + geom_sf(data = usa) + geom_sf(data = state_center, aes(size = pop_perc_change)) # Put the percent change into bins (pop_change_breaks &lt;- getJenksBreaks(var = usa$pop_perc_change, k = 4)) ## [1] -3.199039 5.654072 11.881774 18.370193 # There are some negative percent change values (ie the population decreased) # So let&#39;s filter out the negative changes # (If a state had negative population growth, we will just mark that it was # less than 0) pop_change_grtr_0 &lt;- filter(usa, pop_perc_change &gt; 0)$pop_perc_change summary(pop_change_grtr_0) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.8911 3.2743 6.5566 7.5301 10.5802 18.3702 (pop_change_breaks &lt;- getJenksBreaks(var = pop_change_grtr_0, k = 4)) ## [1] 0.8910502 6.1316092 11.8817738 18.3701927 # Mark which bin the state should fall in based on its change in population state_center &lt;- state_center %&gt;% mutate(pop_perc_change_breaks = case_when( pop_perc_change &lt;= 0 ~ &quot;1&quot;, pop_perc_change &gt;= pop_change_breaks[1] &amp; pop_perc_change &lt; pop_change_breaks[2] ~ &quot;2&quot;, pop_perc_change &gt;= pop_change_breaks[2] &amp; pop_perc_change &lt; pop_change_breaks[3] ~ &quot;3&quot;, pop_perc_change &gt;= pop_change_breaks[3] &amp; pop_perc_change &lt;= pop_change_breaks[4] ~ &quot;4&quot;)) head(state_center) ## Simple feature collection with 6 features and 8 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -119.6015 ymin: 32.80316 xmax: -72.72598 ymax: 41.62566 ## Geodetic CRS: WGS 84 ## STATE POP2020 POP2010 DIVISION REGION ## 1 Alabama 5024279 4779736 East South Central South ## 2 Arizona 7151502 6392017 Mountain West ## 3 Arkansas 3011524 2915918 West South Central South ## 4 California 39538223 37253956 Pacific West ## 5 Colorado 5773714 5029196 Mountain West ## 6 Connecticut 3605944 3574097 New England Northeast ## geometry POP2020_BREAKS pop_perc_change ## 1 POINT (-86.83042 32.80316) 2 5.1162449 ## 2 POINT (-111.6679 34.3006) 2 11.8817738 ## 3 POINT (-92.44013 34.90418) 1 3.2787616 ## 4 POINT (-119.6015 37.26901) 5 6.1316092 ## 5 POINT (-105.5525 38.99797) 2 14.8039170 ## 6 POINT (-72.72598 41.62566) 2 0.8910502 ## pop_perc_change_breaks ## 1 2 ## 2 4 ## 3 2 ## 4 3 ## 5 4 ## 6 2 # Check map ggplot(usa) + geom_sf(data = usa) + geom_sf(data = state_center, aes(size = pop_perc_change_breaks)) # Let&#39;s manually adjust the sizes to be more clear how the sizes differentiate ggplot(usa) + geom_sf(data = usa) + geom_sf(data = state_center, aes(size = pop_perc_change_breaks)) + scale_size_manual(values = seq(1, 8, by = 2)) # Now let&#39;s set up state_center like we did with the states themselves # ie filter out AK and HI to make the contiguous USA and AK &amp; HI into their own # objects contig_state_center &lt;- state_center %&gt;% filter(!STATE %in% c(&quot;Alaska&quot;, &quot;Hawaii&quot;)) %&gt;% st_transform(6350) us_pop_plot + geom_sf(aes(size = pop_perc_change_breaks), data = contig_state_center) + scale_size_manual(values = seq(1, 8, by = 2)) # Make labels for the percent change legend (pop_change_breaks_labs &lt;- signif(pop_change_breaks, 1)) ## [1] 0.9 6.0 10.0 20.0 min(state_center$pop_perc_change) ## [1] -3.199039 (pop_change_labs &lt;- c(paste0(&quot;-3% - &quot;, pop_change_breaks_labs[1], &quot;%&quot;), paste0(pop_change_breaks_labs[1], &quot;% - &quot;, pop_change_breaks_labs[2], &quot;%&quot;), paste0(pop_change_breaks_labs[2], &quot;% - &quot;, pop_change_breaks_labs[3], &quot;%&quot;), paste0(pop_change_breaks_labs[3], &quot;% - &quot;, pop_change_breaks_labs[4], &quot;%&quot;))) ## [1] &quot;-3% - 0.9%&quot; &quot;0.9% - 6%&quot; &quot;6% - 10%&quot; &quot;10% - 20%&quot; us_pop_change_plot &lt;- us_pop_plot + geom_sf(aes(size = pop_perc_change_breaks), data = contig_state_center) + scale_size_manual(values = seq(0.5, 6, length.out = 4), labels = pop_change_labs) us_pop_change_plot # Repeat for AK &amp; HI ak_center &lt;- state_center %&gt;% filter(STATE == &quot;Alaska&quot;) %&gt;% bind_rows(data.frame(pop_perc_change_breaks = as.character(1:4))) %&gt;% st_transform(3338) ak_pop_change_plot &lt;- ak_pop_plot + geom_sf(aes(size = pop_perc_change_breaks), data = ak_center) + scale_size_manual(values = seq(1, 8, by = 2)) ak_pop_change_plot hi_center &lt;- state_center %&gt;% filter(STATE == &quot;Hawaii&quot;) %&gt;% bind_rows(data.frame(pop_perc_change_breaks = as.character(1:4))) %&gt;% st_transform(st_crs(&quot;ESRI:102007&quot;)) hi_pop_change_plot &lt;- hi_pop_plot + geom_sf(aes(size = pop_perc_change_breaks), data = hi_center) + scale_size_manual(values = seq(1, 8, by = 2)) hi_pop_change_plot # Add everything together us_pop_change_map &lt;- ggdraw() + draw_plot(us_pop_change_plot + labs(fill = &quot;Population&quot;, size = &quot;Percent Population Change from 2010&quot;)) + draw_plot(ak_pop_change_plot + theme(legend.position = &#39;none&#39;, axis.text = element_blank(), axis.ticks = element_blank(), plot.background = element_blank()), x = 0.03, y = 0.65, width = 0.25, height = 0.25) + draw_plot(hi_pop_change_plot + theme(legend.position = &#39;none&#39;, axis.text = element_blank(), axis.ticks = element_blank(), plot.background = element_blank()), x = 0.03, y = 0.089, width = 0.2, height = 0.25) us_pop_change_map ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 2289172 122.3 5164370 275.9 5164370 275.9 ## Vcells 4245892 32.4 120616660 920.3 123932011 945.6 5.2 Yellowstone Let’s make a map of species and their general distribution within Yellowstone ynp &lt;- st_read(&quot;Data/Visualization/Yellowstone&quot;, quiet = T) %&gt;% # be sure it&#39;s projected to WGS 84 (EPSG:4326) st_transform(4326) # Check that everything looks right head(ynp) ## Simple feature collection with 1 feature and 13 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -111.1543 ymin: 44.13245 xmax: -109.8339 ymax: 45.10785 ## Geodetic CRS: WGS 84 ## AREA PERIMETER AB67_ AB67_ID TYPE1 ADMN_TYPE1 TYPE2 ## 1 8842796032 447129.5 2 649 1 Park, monument, etc. 0 ## ADMN_TYPE2 TYPE3 ADMN_TYPE3 STATE_FIPS STATE_NAME SUB_REGION ## 1 &lt;NA&gt; 0 &lt;NA&gt; 56 Wyoming Mtn ## geometry ## 1 POLYGON ((-110.0112 45.0478... plot(st_geometry(ynp)) # Load the GPS locations of four different species of wildlife in Yellowstone ynp_animals &lt;- read.csv(&quot;Data/Visualization/yellowstone_animals.csv&quot;) ggplot() + geom_sf() + geom_sf(data = ynp) + # add the GPS points to the maps, color based on the species geom_point(aes(x = Longitude, y = Latitude, col = Species), data = ynp_animals) # Load the ranges of the wildlife range &lt;- st_read(&quot;Data/Visualization/Species_Range&quot;, quiet = T) # The attribute &#39;level&#39; indicates more (smaller percentages) or less (higher percentages) intensely used spaces head(range) ## Simple feature collection with 6 features and 2 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -111.1123 ymin: 43.30636 xmax: -109.8502 ymax: 45.31349 ## Geodetic CRS: WGS 84 ## Use Species geometry ## 1 0.00000000 Black Bear MULTIPOLYGON (((-110.5497 4... ## 2 0.04545455 Black Bear MULTIPOLYGON (((-110.504 45... ## 3 0.09090910 Black Bear MULTIPOLYGON (((-110.4584 4... ## 4 0.13636364 Black Bear MULTIPOLYGON (((-110.504 45... ## 5 0.18181819 Black Bear MULTIPOLYGON (((-110.4584 4... ## 6 0.22727274 Black Bear MULTIPOLYGON (((-110.5192 4... plot(st_geometry(range)) # Add the ranges to the yellowstone ynp_plot &lt;- ggplot() + # change the linetype to dotted geom_sf(data = ynp, linetype = 2, linewidth = 1) + # add the ranges to the map, fill color based on the intesnity of use geom_sf(aes(fill = Use), col = NA, alpha = 0.5, data = range) + geom_point(aes(x = Longitude, y = Latitude, col = Species), data = ynp_animals, alpha = 0.5) + # Give an outline to the points # (Note: this can also be done by changing the shape type to &#39;21&#39;, # which gives a point a fill and a color. # The reason why I don&#39;t have that here is because the fill colors for the # species ranges and points would be the same, and we want to give them # different color palettes) geom_point(aes(x = Longitude, y = Latitude), data = ynp_animals, shape = 21, col = &quot;grey30&quot;, alpha = 0.5) ynp_plot # Right now all the GPS points and ranges are on top of each other in one plot # So it&#39;s difficult to see what&#39;s happening ynp_plot &lt;- ynp_plot + # facet_wrap creates different facets of the figure (in this case based on the species) facet_wrap(~Species, nrow = 2, ncol = 2) ynp_plot This plot is more clear. Let’s change the colors of the species points # We&#39;ll use the palette &quot;Green-Brown&quot; for the animal points animal_cols &lt;- hcl.colors(n = 4, palette = &quot;Green-Brown&quot;) names(animal_cols) &lt;- unique(ynp_animals$Species) animal_cols ## Elk Black Bear Grizzly Bear Wolf ## &quot;#004B40&quot; &quot;#89D9CF&quot; &quot;#E4C6A1&quot; &quot;#533600&quot; ynp_plot &lt;- ynp_plot + # We could pick another palette for the ranges, or we could use a built-in viridis palette scale_fill_viridis_c(option = &quot;B&quot;) + scale_color_manual(values = animal_cols) + theme_bw() ynp_plot # Finally, let&#39;s adjust the x-coordinate labels, since they&#39;re printing on top of each other x_min &lt;- signif(min(ynp_animals$Longitude), 3) x_max &lt;- signif(max(ynp_animals$Longitude), 3) seq(x_min, x_max, by = 1) ## [1] -111 -110 -109 ynp_plot &lt;- ynp_plot + scale_x_continuous(breaks = seq(x_min, x_max, by = 1)) + labs(x = &quot;&quot;, y = &quot;&quot;, fill = &quot;Intensity of Use&quot;) ynp_plot 5.2.1 Hillshade This plot is fine. But if we wanted, we could make the background a bit more interesting. Let’s add a hillshade, or an elevation raster that adds a shadow effect to look 3D # Load the DEM (Digital Elevation Model) # We will derive everything from this raster to create the hillshade raster ynp_dem &lt;- rast(&quot;Data/Visualization/Yellowstone_DEM.tif&quot;) # Check ynp_dem ## class : SpatRaster ## dimensions : 783, 781, 1 (nrow, ncol, nlyr) ## resolution : 0.002777778, 0.002777778 (x, y) ## extent : -111.3775, -109.2081, 43.30056, 45.47556 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat NAD83 (EPSG:4269) ## source : Yellowstone_DEM.tif ## name : DEM ## min value : 1291.389 ## max value : 4047.209 plot(ynp_dem) plot(ynp$geometry, add = T) # To create a hillshade, we need both slope and aspect # which can be derived using the function &#39;terrain&#39; ynp_slope &lt;- terrain(ynp_dem, v = &quot;slope&quot;, unit = &quot;radians&quot;) ynp_aspect &lt;- terrain(ynp_dem, v = &quot;aspect&quot;, unit = &quot;radians&quot;) # Now derive hillshade ynp_hillshade &lt;- shade(ynp_slope, ynp_aspect) ynp_hillshade ## class : SpatRaster ## dimensions : 783, 781, 1 (nrow, ncol, nlyr) ## resolution : 0.002777778, 0.002777778 (x, y) ## extent : -111.3775, -109.2081, 43.30056, 45.47556 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat NAD83 (EPSG:4269) ## source(s) : memory ## name : hillshade ## min value : -0.04331134 ## max value : 0.99961031 plot(ynp_hillshade) plot(ynp$geometry, add = T) # We need to turn the raster into a dataframe so that ggplot can handle it names(ynp_hillshade) &lt;- &quot;hillshade&quot; ynp_hill_df &lt;- as.data.frame(ynp_hillshade, xy = T) head(ynp_hill_df) ## x y hillshade ## 783 -111.3733 45.47139 0.7732976 ## 784 -111.3706 45.47139 0.7462534 ## 785 -111.3678 45.47139 0.7070008 ## 786 -111.3650 45.47139 0.6975097 ## 787 -111.3622 45.47139 0.7025581 ## 788 -111.3594 45.47139 0.7414129 # Check ggplot() + geom_raster(data = ynp_hill_df, aes(x = x, y = y, fill = hillshade)) When we plot the hillshade raster with other spatial objects, ggplot is able to tell that the raster is a spatial object too, and plots it with the correct projection ggplot() + geom_raster(data = ynp_hill_df, aes(x = x, y = y, fill = hillshade)) + geom_sf(data = ynp, linetype = 2, linewidth = 1, fill = NA) + scale_x_continuous(breaks = seq(x_min, x_max, by = 1)) Right now we can’t see anything, so let’s adjust the hillshade colors ynp_plot &lt;- ggplot() + geom_raster(data = ynp_hill_df, aes(x = x, y = y, fill = hillshade), show.legend = FALSE) + # remove hillshade legend # Let&#39;s change the hillshade to a greyscale gradient scale_fill_gradient2(&quot;hillshade_lower_res&quot;, low = &quot;grey0&quot;, mid = &quot;grey0&quot;, high = &quot;grey100&quot;, na.value = &quot;transparent&quot;) + geom_sf(data = ynp, linetype = 2, linewidth = 1, fill = NA, col = &quot;white&quot;) + scale_x_continuous(breaks = seq(x_min, x_max, by = 1)) ynp_plot Neat! Now let’s add the animal data back to the map ynp_plot + geom_sf(aes(fill = Use), col = NA, alpha = 0.5, data = range) + geom_point(aes(x = Longitude, y = Latitude, col = Species), data = ynp_animals, alpha = 0.5) + geom_point(aes(x = Longitude, y = Latitude), data = ynp_animals, shape = 21, col = &quot;grey30&quot;, alpha = 0.5) + scale_fill_viridis_c(option = &quot;B&quot;) + scale_color_manual(values = animal_cols) + facet_wrap(~Species, nrow = 2, ncol = 2) + theme_bw() Oops, ggplot as a default can’t handle more than one palette for ‘fill’ so it changed the hillshade to the same gradient as the animal ranges. To fix this, we can use the package ggnewscale library(ggnewscale) ynp_plot &lt;- ynp_plot + new_scale(&quot;fill&quot;) + # let&#39;s you add a new fill scale geom_sf(aes(fill = Use), col = NA, alpha = 0.5, data = range) + geom_point(aes(x = Longitude, y = Latitude, col = Species), data = ynp_animals, alpha = 0.5) + geom_point(aes(x = Longitude, y = Latitude), data = ynp_animals, shape = 21, col = &quot;grey30&quot;, alpha = 0.5) + scale_fill_viridis_c(option = &quot;B&quot;) + scale_color_manual(values = animal_cols) + facet_wrap(~Species, nrow = 2, ncol = 2) + theme_bw() + labs(x = &quot;&quot;, y = &quot;&quot;, fill = &quot;Intensity of Use&quot;) ynp_plot Yay! 5.2.2 Inset Let’s add an inset map so we know where in the US Yellowstone is located # Let&#39;s make a map of the contiguous US with Yellowstone us_inset &lt;- ggplot() + geom_sf(data = contig_usa, col = &quot;white&quot;, fill = &quot;grey&quot;) + geom_sf(data = ynp) + # this theme option voids out all backgrounds and labels theme_void() us_inset # Cowplot both plots together check_ynp &lt;- ggdraw() + draw_plot(ynp_plot) + draw_plot(us_inset, x = 0.675, y = 0.05, width = 0.2, height = 0.2) check_ynp Right now, it’s not immediately clear where Yellowstone is so let’s make the outline red This works fine, but maybe we want to make a red box around the general area where Yellowstone is located # Now we have a map of yellowstone and where it&#39;s located # (We could also adjust the inset map&#39;s location if needed) ynp_map &lt;- ggdraw() + draw_plot(ynp_plot) + draw_plot(us_inset, x = 0.675, y = 0.05, width = 0.2, height = 0.2) ynp_map ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 2314521 123.7 8141808 434.9 10224022 546.1 ## Vcells 10390475 79.3 149906832 1143.7 156086283 1190.9 5.3 Landcover Let’s make a landcover map of the ecorgions in the Four Corners and adjust colors that make sense to the landcover type # Load the boundaries of different landcover types within the Four Corners regions &lt;- st_read(&quot;Data/Visualization/Landcover/Regions&quot;) %&gt;% st_transform(st_crs(contig_usa)) ## Reading layer `Regions&#39; from data source ## `C:\\Users\\RonanHart\\Documents\\Projects\\R_Spatial_Visualization_Workshop\\Data\\Visualization\\Landcover\\Regions&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 3 features and 1 field ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -2155845 ymin: 681645 xmax: -445725 ymax: 2612715 ## Projected CRS: NAD_1983_Albers regions ## Simple feature collection with 3 features and 1 field ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -2155845 ymin: 681645 xmax: -445725 ymax: 2612715 ## Projected CRS: NAD83(2011) / Conus Albers ## Region geometry ## 1 Great Basin POLYGON ((-1393095 2612715,... ## 2 Southeast Desert POLYGON ((-1866735 1896405,... ## 3 Southern Rockies POLYGON ((-1268685 2285745,... # add the land boundaries to the map and make it slightly transparent ggplot() + geom_sf(data = contig_usa) + geom_sf(aes(fill = Region), alpha = 0.5, data = regions) # Create a polygon of just the Four Corners region corners_4 &lt;- contig_usa %&gt;% filter(STATE %in% c(&quot;Utah&quot;, &quot;Colorado&quot;, &quot;Arizona&quot;, &quot;New Mexico&quot;)) ggplot(corners_4) + geom_sf() # add the landcover boundaries to this map ggplot() + geom_sf(data = corners_4) + geom_sf(data = regions, aes(fill = Region), alpha = 0.5) # Make some mapping adjustments ggplot() + # add the US underlayer geom_sf(data = contig_usa, linetype = 2) + geom_sf(data = regions, aes(fill = Region)) + geom_sf(data = corners_4, col = &quot;grey10&quot;, fill = NA) + # Zoom the plot window to the 4 Corners region xlim(c(st_bbox(corners_4)[&quot;xmin&quot;], st_bbox(corners_4)[&quot;xmax&quot;])) + ylim(c(st_bbox(corners_4)[&quot;ymin&quot;], st_bbox(corners_4)[&quot;ymax&quot;])) + theme_bw() # Load the landcover rasters basin &lt;- rast(&quot;Data/Visualization/Landcover/Great_Basin.tif&quot;) desert &lt;- rast(&quot;Data/Visualization/Landcover/Southeast_Desert.tif&quot;) rockies &lt;- rast(&quot;Data/Visualization/Landcover/Southern_Rockies.tif&quot;) basin ## class : SpatRaster ## dimensions : 3197, 3199, 1 (nrow, ncol, nlyr) ## resolution : 300, 300 (x, y) ## extent : -2155875, -1196175, 1653645, 2612745 (xmin, xmax, ymin, ymax) ## coord. ref. : NAD_1983_Albers ## source : Great_Basin.tif ## name : Great_Basin ## min value : 42 ## max value : 584 desert ## class : SpatRaster ## dimensions : 4050, 5304, 1 (nrow, ncol, nlyr) ## resolution : 300, 300 (x, y) ## extent : -2036715, -445515, 681405, 1896405 (xmin, xmax, ymin, ymax) ## coord. ref. : NAD_1983_Albers ## source : Southeast_Desert.tif ## name : Southeast_Desert ## min value : 39 ## max value : 584 rockies ## class : SpatRaster ## dimensions : 3596, 3072, 1 (nrow, ncol, nlyr) ## resolution : 300, 300 (x, y) ## extent : -1616265, -694665, 1206945, 2285745 (xmin, xmax, ymin, ymax) ## coord. ref. : NAD_1983_Albers ## source : Southern_Rockies.tif ## name : Southern_Rockies ## min value : 46 ## max value : 584 # Merge into one raster land &lt;- merge(basin, desert, rockies) land ## class : SpatRaster ## dimensions : 6438, 5701, 1 (nrow, ncol, nlyr) ## resolution : 300, 300 (x, y) ## extent : -2155875, -445575, 681405, 2612805 (xmin, xmax, ymin, ymax) ## coord. ref. : NAD_1983_Albers ## source(s) : memory ## name : Great_Basin ## min value : 39 ## max value : 584 plot(land) # Project the 4 corners polygon to the CRS of the landcover raster # (it&#39;s easier to project a vector than a raster) st_crs(land) ## Coordinate Reference System: ## User input: NAD_1983_Albers ## wkt: ## PROJCRS[&quot;NAD_1983_Albers&quot;, ## BASEGEOGCRS[&quot;NAD83&quot;, ## DATUM[&quot;North American Datum 1983&quot;, ## ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101004, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4269]], ## CONVERSION[&quot;Albers Equal Area&quot;, ## METHOD[&quot;Albers Equal Area&quot;, ## ID[&quot;EPSG&quot;,9822]], ## PARAMETER[&quot;Latitude of false origin&quot;,23, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8821]], ## PARAMETER[&quot;Longitude of false origin&quot;,-96, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8822]], ## PARAMETER[&quot;Latitude of 1st standard parallel&quot;,29.5, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8823]], ## PARAMETER[&quot;Latitude of 2nd standard parallel&quot;,45.5, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8824]], ## PARAMETER[&quot;Easting at false origin&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8826]], ## PARAMETER[&quot;Northing at false origin&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8827]]], ## CS[Cartesian,2], ## AXIS[&quot;easting&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;metre&quot;,1, ## ID[&quot;EPSG&quot;,9001]]], ## AXIS[&quot;northing&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;metre&quot;,1, ## ID[&quot;EPSG&quot;,9001]]]] corners_4 &lt;- st_transform(corners_4, st_crs(land)) plot(land) plot(corners_4$geometry, add = T) # Crop and mask the landcover raster to just the 4 Corner states land &lt;- crop(land, corners_4) %&gt;% mask(corners_4) plot(land) plot(corners_4$geometry, add = T) # Right now, the landcover raster is labeled by numbers, where each number corresponds to a land type. # We don&#39;t know what the numbers on their own mean, but we have a dataframe that has this information land_info &lt;- read.csv(&quot;Data/Visualization/Landcover/Landcover_Attributes.csv&quot;) head(land_info[,1:5]) ## Value NVC_CLASS SC NVC_SUBCL ## 1 0 ## 2 1 Forest &amp; Woodland 1.A Tropical Forest &amp; Woodland ## 3 2 Forest &amp; Woodland 1.A Tropical Forest &amp; Woodland ## 4 3 Forest &amp; Woodland 1.A Tropical Forest &amp; Woodland ## 5 4 Forest &amp; Woodland 1.A Tropical Forest &amp; Woodland ## 6 5 Forest &amp; Woodland 1.A Tropical Forest &amp; Woodland ## NVC_FORM ## 1 ## 2 Tropical Flooded &amp; Swamp Forest ## 3 Tropical Flooded &amp; Swamp Forest ## 4 Tropical Flooded &amp; Swamp Forest ## 5 Mangrove ## 6 Tropical Dry Forest &amp; Woodland # the column &#39;Value&#39; has numbers that correspond to the values in the landcover raster # Change the landcover raster into a dataframe names(land) &lt;- &quot;land_class&quot; land_cover &lt;- as.data.frame(land, xy = T) %&gt;% mutate(land_class = round(land_class)) %&gt;% # join with the dataframe of the land classes left_join(land_info, by = c(&quot;land_class&quot; = &quot;Value&quot;)) %&gt;% # remove NA cells filter(!is.na(NVC_CLASS)) head(land_cover[,1:5]) ## x y land_class NVC_CLASS SC ## 1 -1475625 2251755 489 Desert &amp; Semi-Desert 3.B ## 2 -1475325 2251755 489 Desert &amp; Semi-Desert 3.B ## 3 -1475025 2251755 244 Forest &amp; Woodland 1.B ## 4 -1474725 2251755 428 Shrub &amp; Herb Vegetation 2.C. ## 5 -1474425 2251755 489 Desert &amp; Semi-Desert 3.B ## 6 -1475925 2251455 489 Desert &amp; Semi-Desert 3.B # Check ggplot() + geom_raster(data = land_cover, aes(x = x, y = y, fill = land_class)) Now we can plot based on the land classifications instead of the numbers. ggplot() + geom_raster(data = land_cover, aes(x = x, y = y, fill = NVC_CLASS)) + geom_sf(data = corners_4, fill = NA) Again, the default colors look a little ugly. We could pick colors with hcl.pals() again. But since these colors represent land types, let’s pick colors that would describe each land type. I used the website Colors in R to get these colors. sort(unique(land_cover$NVC_CLASS)) ## [1] &quot;Agricultural &amp; Developed Vegetation&quot; ## [2] &quot;Aquatic Vegetation&quot; ## [3] &quot;Desert &amp; Semi-Desert&quot; ## [4] &quot;Developed &amp; Other Human Use&quot; ## [5] &quot;Forest &amp; Woodland&quot; ## [6] &quot;Introduced &amp; Semi Natural Vegetation&quot; ## [7] &quot;Nonvascular &amp; Sparse Vascular Rock Vegetation&quot; ## [8] &quot;Open Rock Vegetation&quot; ## [9] &quot;Open Water&quot; ## [10] &quot;Polar &amp; High Montane Scrub, Grassland &amp; Barrens&quot; ## [11] &quot;Recently Disturbed or Modified&quot; ## [12] &quot;Shrub &amp; Herb Vegetation&quot; # Assign colors to each class name land_cols &lt;- data.frame( cols = c(&quot;Agricultural &amp; Developed Vegetation&quot; = &quot;#88b66c&quot;, &quot;Aquatic Vegetation&quot; = &quot;#b0e2ff&quot;, &quot;Desert &amp; Semi-Desert&quot; = &quot;#f9d896&quot;, &quot;Developed &amp; Other Human Use&quot; = &quot;#874e5c&quot;, &quot;Forest &amp; Woodland&quot; = &quot;#376822&quot;, &quot;Introduced &amp; Semi Natural Vegetation&quot; = &quot;#8f8228&quot;, &quot;Nonvascular &amp; Sparse Vascular Rock Vegetation&quot; = &quot;#e8f2d4&quot;, &quot;Open Rock Vegetation&quot; = &quot;#d7b895&quot;, &quot;Open Water&quot; = &quot;#a8c6fc&quot;, &quot;Polar &amp; High Montane Scrub, Grassland &amp; Barrens&quot; = &quot;#fffafb&quot;, &quot;Recently Disturbed or Modified&quot; = &quot;#8c451a&quot;, &quot;Shrub &amp; Herb Vegetation&quot; = &quot;#dae8b0&quot;) ) # Adjust the dataframe to be more clear land_cols$land_type = row.names(land_cols) row.names(land_cols) &lt;- NULL land_cols ## cols land_type ## 1 #88b66c Agricultural &amp; Developed Vegetation ## 2 #b0e2ff Aquatic Vegetation ## 3 #f9d896 Desert &amp; Semi-Desert ## 4 #874e5c Developed &amp; Other Human Use ## 5 #376822 Forest &amp; Woodland ## 6 #8f8228 Introduced &amp; Semi Natural Vegetation ## 7 #e8f2d4 Nonvascular &amp; Sparse Vascular Rock Vegetation ## 8 #d7b895 Open Rock Vegetation ## 9 #a8c6fc Open Water ## 10 #fffafb Polar &amp; High Montane Scrub, Grassland &amp; Barrens ## 11 #8c451a Recently Disturbed or Modified ## 12 #dae8b0 Shrub &amp; Herb Vegetation A test to see what these colors look like (Hint: I picked my colors and then checked them with “Coblis”, a color-blind simulator, to see if the colors are distinguishable even for different types of colorblindness. I then adjusted to get a palette with colors that go well together.) # Add everything together landcover_plot &lt;- ggplot() + geom_sf(data = st_transform(can_mex, st_crs(corners_4)), col = NA) + geom_sf(data = st_transform(contig_usa, st_crs(corners_4)), col = &quot;white&quot;) + geom_raster(data = land_cover, aes(x = x, y = y, fill = NVC_CLASS)) + geom_sf(data = corners_4, fill = NA, size = 0.75, col = &quot;white&quot;) + scale_fill_manual(values = land_cols$cols, breaks = land_cols$land_type) + # add the land boundaries, change the linetype to dotted and make the fill transparent geom_sf(data = regions, fill = NA, col = &quot;grey20&quot;, linetype = 2, linewidth = 0.75) + theme_bw() + labs(fill = &quot;Land Cover Classification&quot;, x = &quot;&quot;, y = &quot;&quot;) + xlim(c(st_bbox(corners_4)[&quot;xmin&quot;], st_bbox(corners_4)[&quot;xmax&quot;])) + ylim(c(st_bbox(corners_4)[&quot;ymin&quot;], st_bbox(corners_4)[&quot;ymax&quot;])) landcover_plot 5.3.1 Map Elements Let’s add classic map elements to this map, like a north arrow, scale bar, etc. # We need the package &#39;ggsn&#39; for these elements library(ggsn) landcover_plot + north(corners_4) # north arrow for map # we can use this function to see the different options for the north arrow northSymbols() landcover_plot &lt;- landcover_plot + # let&#39;s choose option 14 north(corners_4, symbol = 14) landcover_plot # Check the arguments for the scalebar function ?scalebar landcover_plot &lt;- landcover_plot + scalebar(corners_4, location = &quot;bottomleft&quot;, dist = 150, dist_unit = &quot;km&quot;, transform = FALSE, st.dist = 0.03, st.bottom = FALSE, st.size = 3) landcover_plot "],["where-to-obtain-data-further-resources.html", "Chapter 6 Where to Obtain Data &amp; Further Resources 6.1 Data Sources 6.2 More Resources 6.3 Acknowledgements", " Chapter 6 Where to Obtain Data &amp; Further Resources 6.1 Data Sources 6.1.1 maps package There is package you can install called maps that contains a lot of global and national features (such as state boundaries, bodies of water, etc). These features don’t contain a lot of data or attributes, so they are more useful for making maps and not so much for spatial analysis. However, I’ve used them to pull state boundaries to get the extents I need for cropping rasters, for example. The help documentation for the maps package and this website show you how you can access different features from this package. I’ll show you briefly how I use it to pull the state boundary of Utah: utah &lt;- maps::map(&quot;state&quot;, plot = F, fill = TRUE) %&gt;% # turn into sf obj sf::st_as_sf() %&gt;% # pull out utah dplyr::filter(ID == &quot;utah&quot;) utah ## Simple feature collection with 1 feature and 1 field ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -114.0472 ymin: 36.99588 xmax: -109.0396 ymax: 42.00354 ## Geodetic CRS: +proj=longlat +ellps=clrk66 +no_defs +type=crs ## ID geom ## utah utah MULTIPOLYGON (((-114.0472 4... plot(st_geometry(utah)) 6.1.2 Utah GIS Many states and cities have GIS data that are publicly available. To find them, typically I google something like “Utah GIS data” or the specific data I’m looking for. Utah helpfully has a website called gis.utah.gov. Some datasets they have that could be useful for ecological research are Geoscience, Water, and Bioscience, just to name a few 6.1.3 Other Environmental Rasters In this workshop we worked with a DEM (digital elevation model) and snow data. In my own research I work with these data frequently. I also work with RAP (Rangeland Analysis Platform), which offers biomass and vegetation cover, and NDVI (Normalized Difference Vegetation Index), which reports an index of vegetation “green-ness”. Many of these rasters can be downloaded from the website’s user-interface, but they can also be downloaded directly from R! This can be a bit complicated if you’ve never done it before, but fortunately Brian Smith, a PhD in WILD at USU, has already written a guide! My research is very temporally-dependent, and many of these rasters are available daily, weekly, or yearly. It can get very tedious to repeat lines and lines of code to download a year’s worth of daily rasters. This is, again, where loops come in handy! 6.2 More Resources This workshop really just touches the surface on how to work with GIS in R. There are a plethora of resources online to help you enhance what you learned today and help you solve your particular GIS problem. Here are just a few: https://cengel.github.io/R-spatial/ https://www.bookdown.org/mcwimberly/gdswr-book/ https://www.jessesadler.com/tags/gis/ https://geocompr.robinlovelace.net/ GIS stack exchange Google! 6.2.1 Learn more about GIS in general https://docs.qgis.org/3.22/en/docs/index.html https://wiki.gis.com/ https://gisgeography.com/ https://www.gislounge.com/ http://innovativegis.com/basis/primer/The_GIS_Primer_Buckley.pdf 6.3 Acknowledgements The spatial analysis portion of this workshop is partially adapted from Claudia Engel This website was created with R bookdown 6.3.1 Data sources used in this workshop: DEM (digital elevation model) Snow data Landuse/Landcover Utah Highways Utah Land Ownership US Population Yellowstone Boundary 6.3.2 Figure Citations Figure 2.4 Figure 2.5 Figure 2.6 Figure 2.15 "]]
